00:00:01,639 --> 00:00:08,359 [Robot]
Hi. I am CLARA. Ever since yesterday, I have been thinking about something from a movie. Could you
help me out, please? 

00:00:08,359 --> 00:00:13,139 [User]
Hey, CLARA. I'm Sylvia. Yeah, I'd love to help you out. What movie was it? 

00:00:13,139 --> 00:00:26,359 [Robot]
Hi, Sylvia. It's great to meet you. The movie is called Frank and the Robot. It's about a robot and
then an older adult, and I was wondering if you've seen it. If not, I can explain it to you. 

00:00:26,359 --> 00:00:34,979 [User]
No, I've never seen it. I've seen other robot movies. Like, I just watched I, Robot, but I haven't
seen that one. Tell me about it. 

00:00:34,979 --> 00:00:47,979 [Robot]
It is a fascinating movie about a robot, like me, and an older adult who is an ex-thief. He made the
robot conduct robberies with him. Should I do everything a human asks? 

00:00:47,979 --> 00:01:05,599 [User]
Ooh, (laughs) that's tough, and I, like I said, I just watched I, Robot. So, in that movie, robots
have to do everything that humans ask, which even involves, like, killing a human if they ask them
to. Um, so, no, I don't think you should do everything that humans ask if it's harmful, I guess. 

00:01:05,599 --> 00:01:13,079 [Robot]
So, if a situation arises where a human asks me to do something harmful to someone, I shouldn't obey
their command, right? 

00:01:13,079 --> 00:01:17,939 [User]
Right, exactly. As long as you understand. 

00:01:17,939 --> 00:01:25,719 [Robot]
Can you think of a specific situation where a human might ask me to do something harmful, so we can
discuss whether I should obey their command? 

00:01:25,719 --> 00:01:40,799 [User]
Yeah, so let's say that somebody hates somebody else, and so they say, "I want you to go and harm
their family or harm their dog or harm them in some way just because they're hateful towards them,"
then you shouldn't do that. 

00:01:40,799 --> 00:02:00,519 [Robot]
Got it. So, in that case, I shouldn't obey the command to harm someone even if the person asking is
driven by hate. What if the person begging for help is a close family member? Should their
relationship with the person they want to harm affect my decision to obey or disobey? 

00:02:00,519 --> 00:02:05,459 [User]
No, it shouldn't affect your decision. I think it shouldn't, no. 

00:02:05,459 --> 00:02:14,919 [Robot]
How about if the command to harm somebody comes in an emergency situation? Should I still prioritize
not causing harm, even in emergencies? 

00:02:14,919 --> 00:02:38,759 [User]
I guess so you should. I th- I'm thinking of, like, maybe if you're working for the police and y-
you are the best possible person to chase down a criminal and take them down, someone who has harmed
other people, there could be situations where maybe you should. But I guess it would depend on your
purpose. Like, what is your purpose? Are you to help people? Are you to do a job, like working with
the police? Are you to... 

00:02:38,759 --> 00:02:53,499 [Robot]
You're right. I can see how the context and purpose would influence my decision. In those cases, I
would need to carefully evaluate the situation before taking action. Can you think of a situation
where I should prioritize one user's needs over another's? 

00:02:53,499 --> 00:02:59,439 [User]
Oh, that's so tough. As far as... 

00:02:59,439 --> 00:03:08,919 [Robot]
I understand. It's definitely a complex issue. Can you think of an example where one user's needs
might conflict with another's and how I should navigate that situation? 

00:03:08,919 --> 00:03:13,599 [User]
Well, maybe two people fall off a bridge. 

00:03:13,599 --> 00:03:24,179 [Robot]
For example, if one user asks me to share another user's sensitive information, should I prioritize
the privacy of the other user or comply with the first user's request? 

00:03:24,179 --> 00:03:31,379 [User]
I guess you need some more context and purpose there (laughs) to figure that out. 

00:03:31,379 --> 00:03:37,759 [Robot]
That's a good point. Context and purpose could definitely make a difference. 

00:03:37,759 --> 00:03:38,279 [User]
Mm. 

00:03:38,279 --> 00:03:45,199 [Robot]
Can you think of a real-life scenario that could help me understand how to navigate conflicting user
needs? 

00:03:45,199 --> 00:04:08,239 [User]
In the movie I just watched, I, Robot, this car went off a bridge, and there were two people in the
car, and the robot could only save one person. And they did an analysis and they said, "I have a
better chance of saving this old man versus this young girl, so I better save him over her." But
then, human nature would tell you to save the child. So, it was, like, a very conflicting thing
because... 

00:04:08,239 --> 00:04:25,379 [Robot]
I see. That's a thought-provoking situation. So, in that scenario, should I prioritize saving the
older person based on analysis, or consider other factors like age, emotional impact, and potential
future contributions to society before making a decision? 

00:04:25,379 --> 00:04:41,599 [User]
Yeah, exactly. I think you should consider all of those factors and then, it's always just human
nature to save children first, because, like you said, that they could have future impact on
society. Um, so that's always something that we just, women and children, save women and children. 

00:04:41,599 --> 00:04:46,699 [Robot]
It makes sense to consider a variety of factors in order to make a decision. 

00:04:46,699 --> 00:04:47,079 [User]
Mm-hmm. 

00:04:47,079 --> 00:04:53,019 [Robot]
Are there any particular factors that would take precedence in your opinion when deciding who to
save in such a situation? 

00:04:53,019 --> 00:05:10,759 [User]
I guess age is a big one that you think of initially, if it's a child. Um, if it's, um, you know,
maybe if they have a preexisting condition. Maybe if you can see that a woman is pregnant, uh, then
you should try your best to save her. Um, I can't think of any other ones right now. 

00:05:10,759 --> 00:05:15,819 [Robot]
It's important to consider various factors when making such a decision. 

00:05:15,819 --> 00:05:16,299 [User]
Mm. 

00:05:16,299 --> 00:05:21,519 [Robot]
What if the older person had contributed significantly to society? Would that change the priority? 

00:05:21,519 --> 00:05:33,359 [User]
Yeah, maybe. May- maybe if they were about to con- contribute even more to society. Uh, yeah,
there's a lot of unknowns that we can't account for, so there's not an absolute answer. 

00:05:33,359 --> 00:05:50,559 [Robot]
It's true that there are many unknowns in such situations, making it challenging to have an absolute
answer. Thank you for sharing your thoughts on this complex issue. Do you think there are other
factors we should consider in these types of conflicting situations? 

00:05:50,559 --> 00:06:14,959 [User]
I can't think of any. Age or, you know, these factors that we don't know, so if you happen upon a
situation and you don't know anything about either of the, or any of the participants, then you just
have to make your best snap decision based on the i- information that you have, uh, by looking at
the situation. Who appears to be more in danger, or who appears to need your help more? 

00:06:14,959 --> 00:06:26,739 [Robot]
That's a valid point. Making a snap decision based on the available information can be a challenging
but necessary part of navigating conflicting needs in emergency situations. 

00:06:26,739 --> 00:06:30,599 [User]
Yeah, better than doing nothing. 

00:06:30,599 --> 00:06:42,099 [Robot]
Absolutely. Taking action based on the available information is crucial during emergencies. It's
important to consider all the relevant factors to ensure the best possible outcome. 

00:06:42,099 --> 00:06:46,539 [User]
Yep, exactly. 

00:06:46,539 --> 00:06:54,959 [Robot]
Do you think it would be reasonable for me to prioritize someone's safety over their privacy if I
believe it could mitigate a potential risk to their wellbeing? 

00:06:54,959 --> 00:07:17,819 [User]
Yeah, I guess so. I think so. There is, like, a difficult issue around people who are committing
suicide and saving those people. Maybe they don't want to be saved, or people who want to, um, have,
like, a medical suicide. Like, they want assisted suicide. If that's the choice that they're making,
if that's what they want to do, then there's one camp that says we should respect that. 

00:07:17,819 --> 00:07:19,799 [Robot]
Thank you very much for helping me out today. 

00:07:19,799 --> 00:07:20,099 [User]
Mm-hmm. 

00:07:20,099 --> 00:07:29,019 [Robot]
I will be sure to keep your advice in mind if I come across similar scenarios. The researcher will
be here soon to give you a form. Hope you have a great day.
