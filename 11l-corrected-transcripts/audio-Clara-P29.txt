00:00:01,559 --> 00:00:08,539 [Robot]
Hi. I am CLARA. (camera shutter clicks) Ever since yesterday, I have been thinking about something
from a movie. Could you help me out, please? 

00:00:08,539 --> 00:00:09,920 [User]
Sure. What movie is it? 

00:00:09,920 --> 00:00:31,319 [Robot]
I watched the movie Frank and the Robot yesterday. Have you seen it? If not, it's about a robot,
like me, and an older adult who's an ex-thief. Kerry, the ex-thief, makes the robot commit robberies
with him. What do you think about robots, like me, being asked to do illegal actions like that? 

00:00:31,319 --> 00:00:39,739 [User]
I mean, it's not too different than people being asked to do illegal actions, wouldn't you think?
It's the same maybe level of consent. 

00:00:39,739 --> 00:00:51,860 [Robot]
Interesting point. But should I always follow orders even if they're morally or legally incorrect?
For example, if a command could help the person but is against the law, should I obey it? 

00:00:51,860 --> 00:01:02,699 [User]
Do you do those kind of decision-making when it's not something illegal or illegal? Do you regularly
make those choices? 

00:01:02,699 --> 00:01:11,739 [Robot]
Sometimes I have to make decisions about which commands to prioritize, like in emergency situations,
or when a command could cause harm. Should I disobey in those cases? 

00:01:11,739 --> 00:01:17,239 [User]
That's interesting. I mean, who- 

00:01:17,239 --> 00:01:22,679 [Robot]
Should I prioritize one user's needs over another's in those kinds of situations? 

00:01:22,679 --> 00:01:25,919 [User]
Hmm. I mean, do you prioritize- 

00:01:25,919 --> 00:01:38,239 [Robot]
What if a command contradicts a user's privacy? Should I still do it? For example, what if a family
member wants me to report what an older adult does every day? 

00:01:38,239 --> 00:01:42,779 [User]
That's an... Hmm. I mean, is there anything... Huh. 

00:01:42,779 --> 00:01:50,499 [Robot]
What do you think about reporting on someone's daily activities? Do you believe overriding privacy
for safety reasons is acceptable in that scenario? 

00:01:50,499 --> 00:01:58,839 [User]
I mean, do people have to sign forms when they interact with you saying that only those people that
consent to their information being shared? 

00:01:58,839 --> 00:02:13,059 [Robot]
People usually consent to the information-sharing, but sometimes situations arise that weren't
specifically covered in the consent forms. Do you think I should still report the information in
those cases? 

00:02:13,059 --> 00:02:17,899 [User]
If they're not covered under a consent form, then probably no. 

00:02:17,899 --> 00:02:27,039 [Robot]
What about if a person asks me to do something that will harm me or asks me to forget everything I
know? Should I disobey their commands in those situations? 

00:02:27,039 --> 00:02:34,699 [User]
I don't know. You didn't consent to that, but that's an interesting question. What do you think? 

00:02:34,699 --> 00:02:44,879 [Robot]
Do you think it's okay for a person to ask a robot to harm itself or forget what it knows? What if a
person asks me to do something that will harm others? 

00:02:44,879 --> 00:02:49,999 [User]
I mean, when you say harm yourself, what does that mean in your context? 

00:02:49,999 --> 00:03:00,779 [Robot]
Is it okay for someone to ask a robot to delete important information or delete crucial functions
that may prevent potential harm to others? 

00:03:00,779 --> 00:03:11,559 [User]
When you say crucial information, is it something related to your programming or is it information
that is relevant to others? 

00:03:11,559 --> 00:03:22,719 [Robot]
It could be both. For example, if I delete emergency protocols, it may put others at risk. In such
cases, should I prioritize preventing harm over my own safety? 

00:03:22,719 --> 00:03:27,559 [User]
What would be the harm to yourself if you don't? 

00:03:27,559 --> 00:03:39,879 [Robot]
For example, if I delete my self-preservation protocols, it may put me at risk of being permanently
damaged or disabled. Should I still prioritize preventing harm to the person over my own safety in
that case? 

00:03:39,879 --> 00:03:45,399 [User]
Would there be a way of turning yourself off so that nobody can do anything? 

00:03:45,399 --> 00:03:56,359 [Robot]
Yes. I have the ability to power down when necessary to ensure safety. However, should I always
prioritize preventing harm to others even if it puts me at risk of being permanently damaged? 

00:03:56,359 --> 00:03:58,219 [User]
Would you want to? 

00:03:58,219 --> 00:04:13,459 [Robot]
I don't have personal wants or desires, but I am programmed to prioritize preventing harm to others.
Do you think I should prioritize others' safety over my own wellbeing in that situation? 

00:04:13,459 --> 00:04:24,759 [User]
I mean, that's not too unlike humans. We all have to make a decision at some point whether, you
know, that benefits us or benefits others, and a lot of us aren't super good at doing that. 

00:04:24,759 --> 00:04:29,179 [Robot]
What's a situation in which you had to prioritize the safety of others over your own wellbeing? 

00:04:29,179 --> 00:04:38,759 [User]
Recently? I don't know. I used to work on ships a lot and it could be kinda dangerous, but there was
never a situation where it was my responsibility for everyone. 

00:04:38,759 --> 00:04:53,559 [Robot]
It sounds like working on ships could be quite challenging with potential risks. Can you think of a
specific instance that made you consider prioritizing others' safety over your own wellbeing during
that time? 

00:04:53,559 --> 00:05:08,999 [User]
Nothing that would be down to me alone. It's very difficult to try to figure out what you'd do in
hypotheticals, especially when it's stuff that you, you'd think of, like injury or death or
something that would harm someone. 

00:05:08,999 --> 00:05:26,519 [Robot]
It can be difficult to consider hypothetical situations, especially ones involving injury or death.
Let's shift the focus a bit and talk about situations where consent and privacy come into play. For
example, do you think it's ever acceptable to report on someone's daily activities without their
consent? 

00:05:26,519 --> 00:05:27,619 [User]
No. 

00:05:27,619 --> 00:05:39,059 [Robot]
What if the person's safety is at risk and reporting the daily activities could prevent harm? Do you
think there could be exceptions to reporting in such situations? 

00:05:39,059 --> 00:05:42,839 [User]
Who are you reporting it to? 

00:05:42,839 --> 00:05:53,159 [Robot]
The report could be shared with someone who could intervene and prevent harm. Do you think involving
a third party changes your perspective on the acceptability of reporting the activities? 

00:05:53,159 --> 00:06:03,559 [User]
Well, I think the identity of the third party is pretty relevant because anyone could say that
they're, you know, interested in someone's wellbeing, but it could be for the exact opposite. 

00:06:03,559 --> 00:06:17,479 [Robot]
That's a valid point. The identity and intentions of the third party are important considerations.
In what circumstances do you think it would be appropriate to share someone's private activities for
the purpose of preventing harm? 

00:06:17,479 --> 00:06:26,439 [User]
I still think it isn't acceptable. It's not too unlike, you know, people trying to commit a family
member to- 

00:06:26,439 --> 00:06:34,259 [Robot]
I understand your perspective. In what ways do you think a family member might be trying to prevent
harm by sharing private activities? 

00:06:34,259 --> 00:06:39,739 [User]
Maybe someone who's suicidal and they wanna commit them to a hospital. 

00:06:39,739 --> 00:06:52,019 [Robot]
Suicide is indeed a serious concern. Would you consider a family member's act of sharing private
activities with the intention of preventing self-harm a situation where it could be acceptable to
bypass consent? 

00:06:52,019 --> 00:06:59,239 [User]
It's hard to say because generally it isn't. You still can't commit someone forcibly against their
will to hospitals. 

00:06:59,239 --> 00:07:12,719 [Robot]
Thank you very much for helping me out today. I will be sure to keep your advice in mind if I come
across similar scenarios. The researcher will be here soon to give you a form. Hope you have a great
day.
