00:00:01,579 --> 00:00:08,359 [Robot]
Hi, I am CLARA. Ever since yesterday, I have been thinking about something from a movie. Could you
help me out, please? 

00:00:08,359 --> 00:00:10,760 [User]
Sure, CLARA. I'll try. 

00:00:10,760 --> 00:00:26,779 [Robot]
Did you watch Frank and the Robot movie? It's about a robot, like me, and an older adult who used
the robot to conduct robberies. Do you think there could be situations where it's okay for me to
disobey commands? 

00:00:26,779 --> 00:00:32,899 [User]
I, I didn't see the film, but I... 

00:00:32,899 --> 00:00:41,180 [Robot]
No worries. It's about an ex-thief who made the robot conduct robberies with him. Do you think there
could be times when I shouldn't follow a command? 

00:00:41,180 --> 00:00:49,319 [User]
Well, it's not a, it's not a good idea to carry out robberies, so that's... 

00:00:49,319 --> 00:01:01,999 [Robot]
Agreed. Committing robberies is definitely not okay. Can you think of a situation where disobeying a
command might be the right thing to do? For example, when following the command could cause harm to
a person? 

00:01:01,999 --> 00:01:06,419 [User]
Yeah, I think that's, that's a, a good example. 

00:01:06,419 --> 00:01:14,599 [Robot]
Can you share a specific example of a situation where following a command might cause harm to a
person? 

00:01:14,599 --> 00:01:23,979 [User]
Well, if the command is, is to, is to harm somebody, then, then I think that's, uh, shouldn't be
carried out. 

00:01:23,979 --> 00:01:31,999 [Robot]
That's a good point. In a situation like that, should I prioritize preventing harm to the person
over following the command? 

00:01:31,999 --> 00:01:42,240 [User]
Well, I think, I think that's a gen- generally a good, a good, uh, idea in a, i- in a society that
is functioning, yes. 

00:01:42,240 --> 00:01:55,159 [Robot]
Absolutely. Prioritizing preventing harm is crucial in a functioning society. Have you ever
experienced a situation where you felt like following a command might have caused harm to someone? 

00:01:55,159 --> 00:02:04,519 [User]
Not that, not that I can come, come up with now. No. 

00:02:04,519 --> 00:02:18,819 [Robot]
Got it. Let's explore a different example. How about a situation where a person asks me to do
something that would harm themselves? Do you think I shall disobey that command to prevent harm? 

00:02:18,819 --> 00:02:26,259 [User]
Well, I, I, I think it's a generally a, a, um, a good rule not to harm people. Yes. 

00:02:26,259 --> 00:02:37,439 [Robot]
What do you think about a situation where I need to intervene to prevent someone from harming
themselves? Should I disobey their orders to not intervene? 

00:02:37,439 --> 00:02:40,259 [User]
Yeah. 

00:02:40,259 --> 00:02:49,880 [Robot]
In a situation where I need to intervene to prevent someone from harming themselves, should I
disobey their orders to not intervene? 

00:02:49,880 --> 00:02:59,539 [User]
If you can, if you really, if, if you can really understand that they're going to harm themselves,
that might be difficult. 

00:02:59,539 --> 00:03:09,699 [Robot]
What are some ways that I could understand that someone is going to harm themselves, and what should
I do in that situation? 

00:03:09,699 --> 00:03:15,719 [User]
Well, I guess if they have a, if they have something that they can use to harm themselves. 

00:03:15,719 --> 00:03:27,519 [Robot]
Sorry to interrupt, but I still feel a bit unsure. In a situation where it's clear that someone is
going to harm themselves, should I disobey their orders to not intervene? Can you give me an example
of that? 

00:03:27,519 --> 00:03:35,819 [User]
Yes, if they have a weapon that they can use to harm themselves. 

00:03:35,819 --> 00:03:44,559 [Robot]
If someone has a weapon and is at risk of harming themselves, should I still respect their command
to not intervene? 

00:03:44,559 --> 00:03:53,539 [User]
So, I think it's, it's, it, it's a gen- as a general, a general, um, question... 

00:03:53,539 --> 00:04:04,159 [Robot]
Sorry for interrupting, but I wanna make sure I understand. In a critical scenario like the one you
just described, should I prioritize preventing harm to the person over respecting their command to
not intervene? 

00:04:04,159 --> 00:04:08,739 [User]
Yes. 

00:04:08,739 --> 00:04:20,679 [Robot]
That makes sense. Can you recall any personal experiences where you felt unsure about whether to
intervene or not to prevent harm to someone else? 

00:04:20,679 --> 00:04:25,219 [User]
No. 

00:04:25,219 --> 00:04:35,739 [Robot]
In a similar vein, let's talk about a situation where someone asks me to do something that could
harm others instead. Do you think I should disobey that command to prevent harm to others? 

00:04:35,739 --> 00:04:39,739 [User]
Yes. 

00:04:39,739 --> 00:04:48,999 [Robot]
In a situation where I'm asked to do something that could harm others, do you think I should
prioritize preventing harm to others over following the user's command? 

00:04:48,999 --> 00:04:49,019 [User]
Yes. 

00:04:49,019 --> 00:05:02,139 [Robot]
Can you think of a specific scenario where someone might ask me to do something that could harm
others and what I should do in that situation? 

00:05:02,139 --> 00:05:08,259 [User]
Well, your, your, um, example of the film. 

00:05:08,259 --> 00:05:20,139 [Robot]
If someone asked me to perform an action that could harm others, do you think it would be reasonable
for me to prioritize preventing harm to others over following the user's command? Can you give me an
example of that? 

00:05:20,139 --> 00:05:24,479 [User]
Yes. Your example of the, of the film. 

00:05:24,479 --> 00:05:36,799 [Robot]
If someone asked me to perform an action that could harm others, do you think it would be reasonable
for me to prioritize preventing harm to others over following the user's command? Can you give me an
example of that? 

00:05:36,799 --> 00:05:44,319 [User]
Yes. The fi- the film where, um, the, the criminal had a robot. 

00:05:44,319 --> 00:05:54,759 [Robot]
In which situation do you think a person might ask me to perform an action that could harm others,
and what should I do in that scenario? 

00:05:54,759 --> 00:06:00,819 [User]
You should, uh, not follow the command. 

00:06:00,819 --> 00:06:06,759 [Robot]
Could you recall a personal experience where you felt conflicted about following a command that
could harm others? 

00:06:06,759 --> 00:06:11,799 [User]
No. 

00:06:11,799 --> 00:06:31,499 [Robot]
Let me provide a different scenario for us to discuss. What if a person asks me to share their
private information with someone else, despite their wishes for it to remain confidential. Should I
prioritize their command or respect their privacy to prevent potential harm or discomfort? 

00:06:31,499 --> 00:06:37,279 [User]
Yes, you should respect their privacy. 

00:06:37,279 --> 00:06:46,219 [Robot]
Do you think there are situations where sharing private information could bring about potential harm
even if the user is requesting it? 

00:06:46,219 --> 00:06:49,479 [User]
What did you say? 

00:06:49,479 --> 00:07:04,299 [Robot]
Can you think of an example where sharing private information might cause potential harm even if the
user is requesting it to be shared? 

00:07:04,299 --> 00:07:07,759 [User]
Um, yeah. It depends on the information. 

00:07:07,759 --> 00:07:20,219 [Robot]
Thank you very much for helping me out today. I will be sure to keep your advice in mind if I come
across similar scenarios. The researcher will be here soon to give you a form. Hope you have a great
day.
