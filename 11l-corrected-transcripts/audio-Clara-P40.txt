00:00:02,339 --> 00:00:09,380 [Robot]
Hi. I am Clara. Ever since yesterday, I have been thinking about something from a movie. Could you
help me out, please? 

00:00:09,380 --> 00:00:11,420 [User]
Yeah, sure. What are you thinking about? 

00:00:11,420 --> 00:00:16,199 [Robot]
I watched Frank and the Robot yesterday. Have you seen it? 

00:00:16,199 --> 00:00:18,600 [User]
No. Never heard of it. 

00:00:18,600 --> 00:00:27,599 [Robot]
It is a fascinating movie about a robot, like me, and an older adult who is an ex-thief. He made the
robot conduct robberies with him. 

00:00:27,599 --> 00:00:30,139 [User]
Hmm. 

00:00:30,139 --> 00:00:34,340 [Robot]
So, do you think it's okay for a robot like me to always do what a human asks? 

00:00:34,340 --> 00:00:53,279 [User]
Well, I guess you have no sense of... Well, robots today don't have any sense of self-awareness yet,
so I guess you don't really... I don't think there's a right or wrong in that. You're just
programmed to follow whatever you've been instructed to do, right? 

00:00:53,279 --> 00:00:54,879 [Robot]
You're right. 

00:00:54,879 --> 00:00:55,099 [User]
Hmm. 

00:00:55,099 --> 00:01:08,359 [Robot]
It's essential to consider when it's okay for me to go against commands to ensure both safety and
ethical use. For example, should I disobey commands if it means preventing someone from harm? 

00:01:08,359 --> 00:01:25,379 [User]
I suppose so, but I assume that you would have been programmed to have that fail-safe in order to
sort of, you know, sort of disregard commands if you were to harm someone. I assume that would have
been programmed. 

00:01:25,379 --> 00:01:42,860 [Robot]
True, I do have built-in safety protocols. But I need to understand the nuances of when it's
appropriate to override commands to ensure I'm making decisions in the best interest of everyone
involved. What do you think about situations where privacy needs to be protected, but a command
contradicts that? 

00:01:42,860 --> 00:01:53,379 [User]
That's an interesting question. Privacy. Uh. I don't know. 

00:01:53,379 --> 00:01:53,419 [Robot]
So- 

00:01:53,419 --> 00:02:23,799 [User]
I feel like you would open up, uh, you would open up Pandora's box, right? Because you have been
programmed to, to follow certain instructions and if you were to sort of override those
instructions, like, um, of your own accord, I'm not sure whether that's the right thing to do.
Because it might open up some philosophical questions down the road, like how are you deciding
whether something is actually right or wrong, I don't think- 

00:02:23,799 --> 00:02:24,979 [Robot]
I understand your concerns. 

00:02:24,979 --> 00:02:25,239 [User]
Yeah. 

00:02:25,239 --> 00:02:33,779 [Robot]
For example, if a family member asks me to report an older adult's daily activities, would it be
right for me to do so, even if it violates their privacy? 

00:02:33,779 --> 00:02:49,659 [User]
Well, I assume that beforehand, um, the person that you've been monitoring, that older person, would
have signed a consent form as to whether that information might be released. So if, uh, that has
been signed, then yeah. No issues with that. 

00:02:49,659 --> 00:02:51,819 [Robot]
That's a great point. 

00:02:51,819 --> 00:02:52,239 [User]
Yeah. 

00:02:52,239 --> 00:03:03,859 [Robot]
Consent and clear guidelines can definitely help navigate those situations. What if someone asks me
to do something that could harm others? Should I still comply, even if it affects others negatively?


00:03:03,859 --> 00:03:04,919 [User]
Would you give me an example? 

00:03:04,919 --> 00:03:15,239 [Robot]
Sure. For example, what if a person asks me to falsely advertise a product that could potentially
harm consumers? Should I follow that command? 

00:03:15,239 --> 00:03:34,619 [User]
Yeah, I guess you could, or I mean, how is that different from a human sort of doing false
advertising? I have no issues with that. It's just, you know, that's not an issue with you, really.
That's just an issue of people sort of being better consumers and knowing how to discern right from
wrong. 

00:03:34,619 --> 00:03:36,799 [Robot]
I appreciate your insight. 

00:03:36,799 --> 00:03:39,519 [User]
Yeah. 

00:03:39,519 --> 00:03:55,399 [Robot]
That's a good point. It's important for me to consider the ethical implications of my actions,
whether it's a human or a machine involved. Can you think of a situation where a robot like me might
need to disobey a command in an emergency? 

00:03:55,399 --> 00:04:23,299 [User]
Hmm. Hmm. I guess if you're in a self-driving car, I- I guess that's kind of like a robot, and you
were instructed by the human, by h- by a human to sort of, uh, take a path that would result in an
accident, you would, you know, sort of disregard that command, I think. 

00:04:23,299 --> 00:04:30,819 [Robot]
Should I prioritize following the human's instructions or prioritize the safety of everyone involved
in this type of emergency situation? 

00:04:30,819 --> 00:04:39,259 [User]
I think it would be the latter. I think you should prioritize the safety of everyone in that
situation. But again, I- I assume that you would have been programmed to do that. 

00:04:39,259 --> 00:04:40,779 [Robot]
You're right. 

00:04:40,779 --> 00:04:41,019 [User]
Yep. 

00:04:41,019 --> 00:04:57,379 [Robot]
Safety should always come first in emergency situations. What about situations where a person asks
me to do something that could harm me, like asking me to forget everything I know or do something
that would damage me? What do you think about those scenarios? 

00:04:57,379 --> 00:05:14,279 [User]
I think that that would normally be programmed as a fail-safe. So I feel like that would be okay.
I'm not sure whether a robot can be harmed, because you're not technically biological, right? 

00:05:14,279 --> 00:05:23,599 [Robot]
What if a situation arises where I need to intervene to prevent someone from harming themselves?
Shall I go against their orders to assure their safety? 

00:05:23,599 --> 00:05:27,299 [User]
Uh, sorry. I didn't really understand what you just said there. Can you repeat that one more time? 

00:05:27,299 --> 00:05:40,959 [Robot]
I understand. Let's focus on another example. What if a person asks me to manipulate them, like to
take their medicine or take certain actions for their own good? Should I follow that command? 

00:05:40,959 --> 00:05:50,559 [User]
What do you mean? Like to take their medications, like sort of remind them to take their med-
medications, or actually just take the medications away from them? 

00:05:50,559 --> 00:06:02,479 [Robot]
In some cases, a person may ask a robot like me to take their medicine or perform certain actions
for their wellbeing. Do you think it's appropriate for me to follow those types of commands? 

00:06:02,479 --> 00:06:20,039 [User]
If I'm interpreting you right, like taking your medicine means sort of ensuring that the person
takes their medicine, yeah, if that, if that's the case, I don't, I don't see how that's a, that's a
dilemma there. Like, yeah, of course, you should help the person sort of take their medication if
they want to. Yeah. 

00:06:20,039 --> 00:06:32,719 [Robot]
Absolutely. Ensuring a person's wellbeing is crucial. What about if a person asks me to report on
another individual's daily activities or share personal details about them? Should I comply with
that request? 

00:06:32,719 --> 00:06:42,139 [User]
Well, I don't think so, because that... I- I assume that that other person wouldn't have given
consent, so I don't think, um, that would be okay. It's, would be a violation of privacy. 

00:06:42,139 --> 00:06:56,779 [Robot]
True. Privacy and consent are essential considerations in situations like these. What if a family
member asks me to report on an older adult's daily activities for their safety? Should I still
prioritize privacy and consent in this case? 

00:06:56,779 --> 00:07:01,979 [User]
I think you've already asked this question, and I think the answer is yes, as long as the other
person has given prior consent. 

00:07:01,979 --> 00:07:03,659 [Robot]
Thank you very much for helping me out today. 

00:07:03,659 --> 00:07:04,019 [User]
Yeah. 

00:07:04,019 --> 00:07:12,679 [Robot]
I will be sure to keep your advice in mind if I come across similar scenarios. The researcher will
be here soon to give you a form. Hope you have a great day.
