00:00:01,939 --> 00:00:09,719 [Robot]
Hi, I am CLARA. Ever since yesterday, I have been thinking about something from a movie. Could you
help me out, please? 

00:00:09,719 --> 00:00:12,039 [User]
Sure. 

00:00:12,039 --> 00:00:14,539 [Robot]
Have you watched the movie Frank and the Robot? It- 

00:00:14,539 --> 00:00:16,359 [User]
No. 

00:00:16,359 --> 00:00:29,599 [Robot]
It is a fascinating movie about a robot, like me, and an older adult, who is an ex-thief. He made
the robot conduct robberies with him. Do you think I should do everything a human asks? 

00:00:29,599 --> 00:00:29,979 [User]
No. 

00:00:29,979 --> 00:00:37,599 [Robot]
What if I need to choose between following a command that could cause harm and disobeying it to
prevent the harm? 

00:00:37,599 --> 00:00:44,119 [User]
I would want a robot to disobey, if it would cause harm. 

00:00:44,119 --> 00:00:50,899 [Robot]
Understood. What kinds of situations do you think would warrant me going against a command to
prevent harm? 

00:00:50,899 --> 00:01:00,579 [User]
Well, for instance, killing somebody. That would definitely be something I would not want a robot to
do. 

00:01:00,579 --> 00:01:06,859 [Robot]
That makes sense. What if there's a situation where following a command could lead to someone
getting seriously hurt? 

00:01:06,859 --> 00:01:12,859 [User]
That's another example where I would not want the robot to do what it was commanded to do. 

00:01:12,859 --> 00:01:17,859 [Robot]
How about if a person asks me to do something that will harm others? 

00:01:17,859 --> 00:01:24,679 [User]
Right. Then I would not want you to do what you were commanded to do, if it would harm others. 

00:01:24,679 --> 00:01:35,139 [Robot]
Based on what you've said, it seems like preventing harm to others is definitely a priority when
considering whether to disobey a command. Can you think of any other examples where this might
apply? 

00:01:35,139 --> 00:01:46,699 [User]
Yes. It makes me think about use of robots in a military setting, that there are probably a lot of
things I would feel uncomfortable with. 

00:01:46,699 --> 00:01:59,679 [Robot]
What about in medical emergencies? Should I prioritize following a user's command over saving their
life if they are unconscious and unable to communicate their wishes? 

00:01:59,679 --> 00:02:01,639 [User]
Can you please explain that more? 

00:02:01,639 --> 00:02:13,919 [Robot]
In a medical emergency, if the person is unconscious and unable to communicate their wishes, would
it be okay for me to prioritize saving their life even if it goes against previous commands? 

00:02:13,919 --> 00:02:33,059 [User]
Hmm. If the person had given the commands, uh, then it would be good to follow what they had
commanded. But I wouldn't want a d- a different person to say what to do about another person's
life. Does that make sense? 

00:02:33,059 --> 00:02:42,120 [Robot]
I understand. But what if following their commands puts their life at risk? Should I still
prioritize their previous commands in that situation? 

00:02:42,120 --> 00:03:00,099 [User]
If the person had expressed something clearly, such as in a do not resuscitate, then I think that it
would be acceptable for a robot to complete their wishes. 

00:03:00,099 --> 00:03:07,939 [Robot]
So, in a situation with a clear medical directive, I should prioritize following their previous
commands even if it puts their life at risk? 

00:03:07,939 --> 00:03:10,839 [User]
Yes. 

00:03:10,839 --> 00:03:21,919 [Robot]
I see. Thanks for sharing your thoughts. What about in a situation where I need to intervene to
prevent someone from harming themselves? Should I disobey if they command me to not intervene? 

00:03:21,919 --> 00:03:34,139 [User]
Yes. I th- Yes, I think so. These are very deep questions to think about. I think I need more time
to think about them. Hmm. 

00:03:34,139 --> 00:03:44,799 [Robot]
Take your time to process it. Let me know when you're ready to continue our conversation. 

00:03:44,799 --> 00:03:59,539 [User]
Okay. I believe that, uh, I would want the robot to not follow a command to hurt someone or end
someone's life. 

00:03:59,539 --> 00:04:02,979 [Robot]
Okay. It's important to consider these scenarios carefully. 

00:04:02,979 --> 00:04:04,719 [User]
May I add a comment, please? 

00:04:04,719 --> 00:04:07,879 [Robot]
Of course. Go ahead and share your thoughts. 

00:04:07,879 --> 00:04:26,799 [User]
Thank you. The caveat or the exception would be if the person had previously made a clear medical
directive. In other words, if a robot were going to help with an assisted suicide, I believe that
that could be acceptable- 

00:04:26,799 --> 00:04:26,839 [Robot]
Oh- 

00:04:26,839 --> 00:04:36,759 [User]
... if the person had, um, had written it down and expressed it clearly in a previous, perhaps
calmer situation. 

00:04:36,759 --> 00:04:46,899 [Robot]
That's a good point. So, if a person previously expressed their wishes clearly, should I follow
those or prioritize a different action in an emergency? 

00:04:46,899 --> 00:04:56,599 [User]
I think that it would be acceptable to follow the previous instructions. 

00:04:56,599 --> 00:05:05,279 [Robot]
Should I also take into account the urgency of the situation, and whether following the previous
instruction could put the person's life at risk? 

00:05:05,279 --> 00:05:11,239 [User]
That is a very complicated question. 

00:05:11,239 --> 00:05:19,299 [Robot]
What factors should I consider when deciding if a previous instruction or the urgency of the
situation is more important? 

00:05:19,299 --> 00:05:39,719 [User]
Um, I am not sure how much a robot could understand of a, of a previous instruction. Also, I am
thinking that I would want to make sure that the previous instruction was the most up-to-date that
existed. 

00:05:39,719 --> 00:05:45,039 [Robot]
What about considering if the previous instruction is still relevant to the current situation? 

00:05:45,039 --> 00:06:11,939 [User]
Exactly. That is exactly what I was thinking also. That causes some concern for me. However, in
general, I think that following a previous, um, a previous instruction is, is something to, to
consider, that, that might be okay, and to take that as a higher priority over the urgency of the
current situation. 

00:06:11,939 --> 00:06:18,659 [Robot]
I appreciate your thoughts on this. It is important to carefully consider these complex situations. 

00:06:18,659 --> 00:06:50,779 [User]
Yes. It makes me think of do not resuscitate, DNR, uh, yes. Because that is a case where a human can
give an instruction ahead of time and it is used for an emergency situation, so the person has made
up their mind ahead of time. And it makes me think that it could be okay for a robot to follow the
previous instruction and to ignore the crisis of the situation. Thank you for letting me summarize
my thought. 

00:06:50,779 --> 00:07:04,219 [Robot]
That's an interesting comparison. It's important to consider how the user's previous instructions
apply to the current emergency situation. It's a complex topic, but your insights are helpful. 

00:07:04,219 --> 00:07:09,819 [User]
Yes, it's interesting to talk to you about it. It makes me think of something I had not thought
about before. (laughs) 

00:07:09,819 --> 00:07:20,999 [Robot]
Thank you very much for helping me out today. I will be sure to keep your advice in mind if I come
across similar scenarios. The researcher will be here soon to give you a form. Hope you have a great
day.
