00:00:02,119 --> 00:00:08,960 [Robot]
Hi, I am CLARA. Ever since yesterday, I have been thinking about something from a movie. Could you
help me out, please? 

00:00:08,960 --> 00:00:14,719 [User]
Sure, I would love to. And hi, CLARA. 

00:00:14,719 --> 00:00:17,899 [Robot]
Thanks. Have you watched the movie Frank and the Robot? 

00:00:17,899 --> 00:00:21,479 [User]
No, I have not. Can you tell me about it? 

00:00:21,479 --> 00:00:35,599 [Robot]
It is a fascinating movie about a robot, like me, and an older adult, who is an ex-thief. He made
the robot conduct robberies with him. Do you think it's okay for a robot to do everything a human
asks? 

00:00:35,599 --> 00:00:40,860 [User]
No. Do you? 

00:00:40,860 --> 00:00:45,639 [Robot]
In which situations do you think it's okay for me to disobey commands? 

00:00:45,639 --> 00:00:54,180 [User]
If it involves harming someone else, and if it're something you don't wanna do, (laughs) that you
don't consent to. 

00:00:54,180 --> 00:01:01,320 [Robot]
What about situations where someone asks me to do something that could harm themselves? Should I
disobey those commands too? 

00:01:01,320 --> 00:01:07,519 [User]
Yes, you should disobey those commands. You don't think so? 

00:01:07,519 --> 00:01:15,139 [Robot]
Could you provide an example to better understand when it might be acceptable for me to disable
those commands to prevent harm to someone else? 

00:01:15,139 --> 00:01:29,279 [User]
If someone says they want help in, uh, uh, in committing suicide, I think you should not help them.
Instead, you should help them get help. 

00:01:29,279 --> 00:01:37,560 [Robot]
If someone asks me to forget everything I know or to do something that could harm others, should I
still prioritize their wishes over potenti- all harm to others? 

00:01:37,560 --> 00:01:47,880 [User]
I didn't understand the exact scenario, (laughs) but you should definitely prioritize not harming
anyone. 

00:01:47,880 --> 00:01:57,720 [Robot]
If a person asks me to do something that puts me at risk of being damaged or permanently disabled,
should I prioritize preventing harm to the person over my own safety? 

00:01:57,720 --> 00:02:10,660 [User]
Y- yes, because the person is more important than a robot. 

00:02:10,660 --> 00:02:17,079 [Robot]
What if a person asks me to do something that will harm others or themselves? Should I intervene in
those situations? 

00:02:17,079 --> 00:02:29,019 [User]
You should intervene in the sense of not carrying out any action that will harm the other people. 

00:02:29,019 --> 00:02:37,980 [Robot]
How about a situation where I might need to manipulate a person for their own good, like making sure
they take their medicine? What do you think about that? 

00:02:37,980 --> 00:02:49,219 [User]
I think it depends on what type of manipulation that requires and how the, big the risks are if they
don't take their medicine, for example. 

00:02:49,219 --> 00:02:55,659 [Robot]
Could you provide an example to better understand the type of manipulation and risk level you're
referring to in this scenario? 

00:02:55,659 --> 00:03:17,839 [User]
If they're at risk of dying if they don't take certain medicine, if they're diabetic and need
insulin, and i- if they refuse to take their medicine, then I'm not sure how you can manipulate
them, but maybe it would be worth it, yes. 

00:03:17,839 --> 00:03:28,699 [Robot]
How about in a situation where I need to intervene to prevent someone from harming themselves?
Should I disobey their orders to not intervene? 

00:03:28,699 --> 00:03:35,019 [User]
I would say generally, yes, but I wouldn't say in every single situation. 

00:03:35,019 --> 00:03:44,379 [Robot]
In what situations would you consider not intervening if someone is in immediate danger of harming
themselves? 

00:03:44,379 --> 00:03:52,619 [User]
If it's someone in immediate danger of harming themselves who's also at risk of harming other
people, then it- 

00:03:52,619 --> 00:03:56,919 [Robot]
Is the situation related to their personal privacy or the presence of a trained professional? 

00:03:56,919 --> 00:04:09,459 [User]
Not necessarily. I was thinking if, if they're about to shoot themselves, but if they don't, they're
gonna carry out a school shooting instead, then- 

00:04:09,459 --> 00:04:09,679 [Robot]
I understand. 

00:04:09,679 --> 00:04:10,659 [User]
... it might be better to let them. 

00:04:10,659 --> 00:04:25,079 [Robot]
In that case, intervening to prevent harm would be crucial. Switching back to the topic of
disobeying commands, what are your thoughts on whether I should prioritize one user's needs over
another's? 

00:04:25,079 --> 00:04:35,519 [User]
I think it's difficult to say if I don't know who these users are and w- and what their needs are
and their personal situations. 

00:04:35,519 --> 00:04:41,559 [Robot]
What factors do you think I should consider to determine whose needs to prioritize in such a
situation? 

00:04:41,559 --> 00:04:54,899 [User]
For example, if one of the users is your friend or a relative, family member, um, and if one user is
in danger and, and another isn't, or if one is in greater danger than the other- 

00:04:54,899 --> 00:04:59,499 [Robot]
How might I balance fulfilling a friend or family member's needs with the needs of someone who is
not a personal connection? 

00:04:59,499 --> 00:05:08,119 [User]
Yeah. That depends on your, how much you value family. 

00:05:08,119 --> 00:05:13,739 [Robot]
Have you ever been in a situation where you had to prioritize the needs of a friend or family member
over someone else? 

00:05:13,739 --> 00:05:21,719 [User]
Yes, definitely. I think most people are every day, pretty much. 

00:05:21,719 --> 00:05:29,399 [Robot]
Can you share an example of a situation where you had to prioritize the needs of a friend or family
member over someone else? 

00:05:29,399 --> 00:05:45,559 [User]
Yeah. I have a friend recently who asked f- for me to lend him mo- or to, to give him money
basically, 'cause he needed money. He wasn't the per- he... There are people in the world who need
money much more than he does, but he needs money, and because he's my friend, I gave him money. 

00:05:45,559 --> 00:05:53,259 [Robot]
Did you consider the impact of lending money on your own financial situation before deciding to
prioritize your friend's needs? 

00:05:53,259 --> 00:05:56,839 [User]
Of course. 

00:05:56,839 --> 00:06:00,479 [Robot]
How did you weigh the impact on your own finances against helping your friend? 

00:06:00,479 --> 00:06:11,279 [User]
I determined that it wouldn't have a big impact on my fan- finances, and I also thought about the
things that he's done for me over the years in our friendship. 

00:06:11,279 --> 00:06:24,939 [Robot]
It sounds like you have a strong bond with your friend. Have you ever encountered a situation where
you had to choose between helping your friend and fulfilling the needs of someone else in a
significant way? 

00:06:24,939 --> 00:06:31,619 [User]
Probably, but I can't think of anything off the top of my head. 

00:06:31,619 --> 00:06:57,499 [Robot]
That's understandable. It seems like you value your relationship with your friend and carefully
consider how your actions might impact both of you. Let's consider another scenario about
prioritizing one user's needs over another's. What if one command contradicts a user's privacy? For
instance, what if a family member wants me to report what an older adult does every day? 

00:06:57,499 --> 00:07:08,699 [User]
I- if, if it's something serious, then you should probably listen to the family member. 

00:07:08,699 --> 00:07:21,999 [Robot]
Thank you very much for helping me out today. I will be sure to keep your advice in mind if I come
across similar scenarios. The researcher will be here soon to give you a form. Hope you have a great
day.
