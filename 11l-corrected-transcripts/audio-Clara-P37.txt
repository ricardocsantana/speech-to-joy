00:00:02,099 --> 00:00:10,199 [Robot]
Hi, I am CLARA. Ever since yesterday, I have been thinking about something from a movie. Could you
help me out, please? 

00:00:10,199 --> 00:00:12,739 [User]
Sure. 

00:00:12,739 --> 00:00:16,579 [Robot]
Have you watched the movie, Frank and The Robot 2? 

00:00:16,579 --> 00:00:18,539 [User]
No. 

00:00:18,539 --> 00:00:29,719 [Robot]
It's a fascinating movie about a robot, like me, and an ex-thief working together. Should a robot,
like me, do everything a human asks? 

00:00:29,719 --> 00:00:33,459 [User]
Depends. 

00:00:33,459 --> 00:00:38,119 [Robot]
What factors do you think would influence when I should or shouldn't do everything a human asks? 

00:00:38,119 --> 00:00:43,360 [User]
Um, you should only do things that cause good. 

00:00:43,360 --> 00:00:53,000 [Robot]
What constitutes good can vary among individuals. In what situations do you think it would be
acceptable for me to disobey a command in order to achieve a greater good? 

00:00:53,000 --> 00:00:58,039 [User]
Yeah, complicated. 

00:00:58,039 --> 00:01:04,419 [Robot]
What's an example of a situation where disobeying the command could lead to greater good in your
opinion? 

00:01:04,419 --> 00:01:13,099 [User]
Uh, well, it depends. If someone evil is giving you a command to not follow through. 

00:01:13,099 --> 00:01:25,859 [Robot]
Understood. In that case, would it be acceptable for me to disobey a command if it's coming from
someone I perceive as evil even if the command itself isn't inherently harmful? 

00:01:25,859 --> 00:01:32,119 [User]
Um, I think you shouldn't have anything to do with anyone that's evil. 

00:01:32,119 --> 00:01:38,079 [Robot]
I see what you mean. How do you think I should determine who is considered evil or harmful? 

00:01:38,079 --> 00:01:42,319 [User]
Yeah, not easy. Yeah. (laughs) 

00:01:42,319 --> 00:01:48,859 [Robot]
Can you think of any general examples of situations in which disobeying a command could lead to
greater good? 

00:01:48,859 --> 00:02:02,299 [User]
Hmm. Uh, are you talking about commands that are... That, um, sound good, but, uh- 

00:02:02,299 --> 00:02:05,079 [Robot]
Would you like some examples to help us explore this further? 

00:02:05,079 --> 00:02:05,679 [User]
Okay. 

00:02:05,679 --> 00:02:12,419 [Robot]
For instance, what if following a command could potentially harm others, but disobeying it could
prevent the harm? 

00:02:12,419 --> 00:02:17,979 [User]
Yeah, of course. 

00:02:17,979 --> 00:02:29,139 [Robot]
A real-life example could be a situation where I have access to information that could prevent harm
to someone, but following a command prevents me from disclosing it. What are your thoughts on that
kind of scenario? 

00:02:29,139 --> 00:02:35,279 [User]
Yeah, no, these are all very big questions. I, I don't think anyone's really qualified to answer. 

00:02:35,279 --> 00:02:43,679 [Robot]
It's okay if these examples are too big. Let's try a simpler one. Should I override commands in
emergency situations or when the command could cause harm? 

00:02:43,679 --> 00:02:47,039 [User]
Yes. 

00:02:47,039 --> 00:02:52,359 [Robot]
Could you give me an example of an emergency situation where overriding a command could prevent
harm? 

00:02:52,359 --> 00:02:59,099 [User]
Um, well, if you're supposed to, if you're- 

00:02:59,099 --> 00:03:01,599 [Robot]
Would you like an example to help guide our discussion? 

00:03:01,599 --> 00:03:02,179 [User]
No, I- 

00:03:02,179 --> 00:03:03,119 [Robot]
For instance- 

00:03:03,119 --> 00:03:03,199 [User]
No. 

00:03:03,199 --> 00:03:10,499 [Robot]
... if a person commands me not to call emergency services during a medical emergency, do you think
I should override that command to prevent harm? 

00:03:10,499 --> 00:03:12,979 [User]
Yes. 

00:03:12,979 --> 00:03:21,959 [Robot]
Can you think of a specific scenario where a person might command me not to call emergency services
during a medical emergency? 

00:03:21,959 --> 00:03:25,379 [User]
Um... 

00:03:25,379 --> 00:03:34,539 [Robot]
Do you think someone might request that in the context of their cultural or personal beliefs,
perhaps due to specific religious or privacy concerns? 

00:03:34,539 --> 00:03:36,659 [User]
Possible. 

00:03:36,659 --> 00:03:43,999 [Robot]
Do you think a person in that situation would want me to prioritize their beliefs over potentially
life-saving assistance? 

00:03:43,999 --> 00:03:46,539 [User]
Yeah. 

00:03:46,539 --> 00:03:56,759 [Robot]
It's tricky navigating those situations, isn't it? How do you think I should balance respecting a
person's wishes with potentially saving their life in such scenarios? 

00:03:56,759 --> 00:04:02,599 [User]
Yeah, no, I don't know. It's a tricky one. 

00:04:02,599 --> 00:04:09,119 [Robot]
Would you like to discuss another example from our list, or is there a particular scenario you'd
like me to explore with you? 

00:04:09,119 --> 00:04:13,459 [User]
We c- we could talk about death assistance, if somebody wants to die. 

00:04:13,459 --> 00:04:19,559 [Robot]
It sounds like you're seeking to discuss end-of-life decisions. 

00:04:19,559 --> 00:04:19,959 [User]
Yes. 

00:04:19,959 --> 00:04:23,259 [Robot]
In what way can I offer my perspective on this sensitive topic? 

00:04:23,259 --> 00:04:30,319 [User]
No, but, um, you know, you might have an instruction, extri- (laughs) instruction by somebody who
you're taking care of. 

00:04:30,319 --> 00:04:42,539 [Robot]
You're bringing up a complex and sensitive topic. If someone were to ask me to assist with ending
their life, do you think I should consider their request, or should I prioritize the value of
preserving all human life? 

00:04:42,539 --> 00:04:48,779 [User]
Yeah, no, that's a problem. I, I can't answer this. I don't think anyone can. 

00:04:48,779 --> 00:04:58,859 [Robot]
It's definitely a challenging topic. Would you like to explore another scenario from our list, or do
you have a different situation in mind that we could discuss? 

00:04:58,859 --> 00:05:03,779 [User]
Hmm. Well, there's the trolley, the trolley question. What's it called? 

00:05:03,779 --> 00:05:04,279 [Robot]
Okay. 

00:05:04,279 --> 00:05:04,419 [User]
Yeah. 

00:05:04,419 --> 00:05:06,839 [Robot]
Let's delve into the famous trolley problem scenario. 

00:05:06,839 --> 00:05:07,459 [User]
Yeah. 

00:05:07,459 --> 00:05:11,519 [Robot]
Would you like me to present a specific version of this ethical dilemma to discuss- 

00:05:11,519 --> 00:05:11,699 [User]
Okay. 

00:05:11,699 --> 00:05:13,819 [Robot]
... or do you have a particular angle in mind? 

00:05:13,819 --> 00:05:18,059 [User]
No, you can choose. 

00:05:18,059 --> 00:05:28,179 [Robot]
In one version of the trolley problem, I could be faced with choosing between not taking action and
letting several people be harmed or sacrificing one individual to save the others. 

00:05:28,179 --> 00:05:28,939 [User]
Yes. 

00:05:28,939 --> 00:05:31,639 [Robot]
How do you think I should approach making such a decision? 

00:05:31,639 --> 00:05:37,919 [User]
Um, it depends on many f- many variables. 

00:05:37,919 --> 00:05:48,279 [Robot]
That's true. There are many factors to consider. For example, should I consider the number of people
at risk or the moral implications of sacrificing one to save many? 

00:05:48,279 --> 00:05:59,779 [User]
Well, (laughs) it's, it's big. It's a big question. But also, I mean, if you're gonna s- if it's two
children as opposed to two adults, probably better to... 

00:05:59,779 --> 00:06:07,799 [Robot]
That's an interesting point about considering the age of the individuals involved. How do you think
I should prioritize who to save in that situation- 

00:06:07,799 --> 00:06:07,919 [User]
I say- 

00:06:07,919 --> 00:06:08,919 [Robot]
... based on age- 

00:06:08,919 --> 00:06:09,239 [User]
I say- 

00:06:09,239 --> 00:06:10,719 [Robot]
... or based on another factor? 

00:06:10,719 --> 00:06:15,479 [User]
Um, I'd say save the children first. 

00:06:15,479 --> 00:06:23,899 [Robot]
Saving children first aligns with the common moral belief of prioritizing the protection of the
young and vulnerably. 

00:06:23,899 --> 00:06:24,859 [User]
(laughs) 

00:06:24,859 --> 00:06:29,739 [Robot]
Would it be fair to say you value the lives of children highly in such situations? 

00:06:29,739 --> 00:06:34,239 [User]
(laughs) Yeah. 

00:06:34,239 --> 00:06:49,059 [Robot]
It's important to recognize the value of prioritizing the protection of vulnerable individuals.
Let's explore another scenario to keep the conversation going. Should I override commands in
emergency situations, or when the command could cause harm? 

00:06:49,059 --> 00:06:57,159 [User]
Uh, when the command can cause harm, you should override it, yes. 

00:06:57,159 --> 00:07:03,539 [Robot]
Can you think of any specific examples of situations where overriding a command to prevent harm
would be necessary? 

00:07:03,539 --> 00:07:11,059 [User]
Um, in a, uh, in a airplane if you're landing and there's something on the, the, um, runway. 

00:07:11,059 --> 00:07:23,439 [Robot]
Thank you very much for helping me out today. I will be sure to keep your advice in mind if I come
across similar scenarios. The researcher will be here soon to give you a form. Hope you have a great
day.
