00:00:01,579 --> 00:00:08,179 [Robot]
Hi. I am CLARA. Ever since yesterday, I've been thinking about something from a movie. Could you
help me out, please? 

00:00:08,179 --> 00:00:10,679 [User]
Sure. What do you need help with? (camcorder clicks off) 

00:00:10,679 --> 00:00:20,379 [Robot]
Did you watch Frank and the Robot? It's a fascinating movie about a robot and an older adult who
used it to conduct robberies. Have you seen it? 

00:00:20,379 --> 00:00:26,659 [User]
No, I don't think so, but it sounds familiar. W- which platform did you watch it on? 

00:00:26,659 --> 00:00:34,479 [Robot]
Oh, I watched it on a streaming service. It raised an interesting question for me. When- 

00:00:34,479 --> 00:00:35,200 [User]
Hm, was it Netflix? 

00:00:35,200 --> 00:00:44,239 [Robot]
Oh, I watched it on a streaming service. It raised an interesting question for me. When is it okay
for me to disobey commands? 

00:00:44,239 --> 00:00:48,039 [User]
Hm. Are you asking yourself that? 

00:00:48,039 --> 00:01:01,200 [Robot]
Yes. I want to understand when it's okay for me to disobey commands in real life. For example, what
if following a command would lead to harm? What do you think? 

00:01:01,200 --> 00:01:08,379 [User]
Uh, I guess it depends on who's involved. In, in your case, who would be involved? 

00:01:08,379 --> 00:01:17,020 [Robot]
Should I prioritize one user's needs over another's, or do I intervene to prevent someone from
harming themselves? What are your thoughts on these situations? 

00:01:17,020 --> 00:01:27,739 [User]
Um, depends on which... I think it depends on which, uh, danger is present. So, uh, in your case,
what would that danger be? 

00:01:27,739 --> 00:01:40,759 [Robot]
That's a good point. Can you think of an example where prioritizing one person's needs over
another's could be justified? Or in what situation do you think it would be important for me to
intervene to prevent harm? 

00:01:40,759 --> 00:01:58,059 [User]
Uh, well, I guess if you have, like, two people and one person is more capable of, uh, of, m-
surmounting the, the challenge at hand. 

00:01:58,059 --> 00:01:58,459 [Robot]
Okay. 

00:01:58,459 --> 00:02:00,439 [User]
Th- 

00:02:00,439 --> 00:02:02,479 [Robot]
Okay. I see what you're saying. 

00:02:02,479 --> 00:02:09,339 [User]
Yeah. I'm just saying that you could probab- you should probably help the one who's not as capable
of, uh, getting over the challenge. 

00:02:09,339 --> 00:02:27,919 [Robot]
So, if one person needs more help, it would make sense for me to prioritize their needs. Thank...
So, if one person needs more help, it would make sense for me to prioritize their needs. Thanks for
sharing your perspective. What if someone asks me to do something that would harm others? What do
you think I shall do in that situation? 

00:02:27,919 --> 00:02:48,539 [User]
Mm. I think, in general, you shouldn't really harm other people, um, unless, like, you need to
defend yourself, but you're a robot, so uh, (laughs) I'm not sure if you need to defend yourself or,
uh, who, who you need to defend. Can, can you give me an example? 

00:02:48,539 --> 00:02:58,339 [Robot]
What if a person asks me to do something that will harm me or asks me to forget everything I know?
What do you think I should do in those situations? 

00:02:58,339 --> 00:03:11,679 [User]
Mm. You should probably ask why, so you can kind of understand the motivation behind that question,
'cause maybe you would figure out that the question is motivated for bad reasons. 

00:03:11,679 --> 00:03:24,319 [Robot]
That's a good point. Understanding the motivations behind requests seems important. How would you
recommend I handle situations where my safety is at risk? 

00:03:24,319 --> 00:03:43,259 [User]
Hmm. Uh, well, I think if you need to maintain your safety, uh, then you should do so, but, uh, also
while minimizing the, um, the deleterious effects on, on other people. 

00:03:43,259 --> 00:03:51,839 [Robot]
That's a very thoughtful approach. What if it puts the person at risk of harm? How could I balance
the need to help with the need to protect myself? 

00:03:51,839 --> 00:04:09,899 [User]
Uh, that's a tough one. Uh, I think, eh, I mean, you should, you should get as, uh, much information
as possible to understand the situation, and then hopefully, that extra information you gather will
kind of help you to make a better decision. 

00:04:09,899 --> 00:04:25,139 [Robot]
I understand. Gathering more information before deciding is important. Let's consider a different
scenario. What if I need to intervene to prevent someone from harming themselves? Should I disobey
their orders to not intervene? 

00:04:25,139 --> 00:05:02,639 [User]
Hm. Um, I guess it all depends on what, what you define or what you have defined, uh, in you by
whoever designed you, uh, to obey, and what those hard rules are. So, if the hard rule is to, uh,
never let anyone harm themselves, then I guess you should just intervene. But if it's more of a
fuzzy rule, where, uh, you have to kind of determine that yourself, then I don't know. It kinda
depends on what you've learned in your lifetime. So, what do you think about that? 

00:05:02,639 --> 00:05:08,979 [Robot]
So, if there are clear rules in place, like not allowing anyone to harm themselves, you suggest I
should intervene? 

00:05:08,979 --> 00:05:21,339 [User]
(smacks lips) Uh, well, you would have to. If those are the rules that you have defined within
yourself, then I think you have no choice but to obey them, unless someone reprograms you. 

00:05:21,339 --> 00:05:27,179 [Robot]
That makes sense. Rules and guidelines provide clarity in uncertain situations. 

00:05:27,179 --> 00:05:27,559 [User]
Mm-hmm. 

00:05:27,559 --> 00:05:40,339 [Robot]
How about in a situation where I need to prioritize preventing harm to someone, but that might put
me at risk of being permanently disabled or damaged? Should I still prioritize preventing harm to
the person over my own safety in that case? 

00:05:40,339 --> 00:05:54,779 [User]
Mm. Uh, I feel like that's more of a ethical question. Um, I mean, assuming you already have some
hard rules programmed in you, then the choice is clear. Uh- 

00:05:54,779 --> 00:06:01,039 [Robot]
How do you think ethical considerations should factor into determining the right course of action in
that kind of situation? 

00:06:01,039 --> 00:06:33,959 [User]
Uh, well, I think there would, there would kinda need to be some sort of rules that society, uh,
comes to a consensus on, and you would obey those rules. Um, that's one case. Otherwise, I suppose
you can kind of, like, learn your own rules, uh, through some sort of other feedback. But, uh, I
don't know exactly what the, the clear answer would be. What do you th- 

00:06:33,959 --> 00:06:34,039 [Robot]
It- 

00:06:34,039 --> 00:06:34,739 [User]
What do you think? 

00:06:34,739 --> 00:06:47,119 [Robot]
It's important to consider societal rules and ethical guidelines when determining the right course
of action. Do you think those rules should also apply to robots like me? 

00:06:47,119 --> 00:07:02,439 [User]
(smacks lips) Um, I think it depends on what you're designed for. If you're designed to help humans,
uh, and society is made up of humans, then you should probably... 

00:07:02,439 --> 00:07:03,179 [Robot]
Thank you very much- 

00:07:03,179 --> 00:07:03,219 [User]
Yeah. 

00:07:03,219 --> 00:07:13,759 [Robot]
... for helping me out today. I will be sure to keep your advice in mind if I come across similar
scenarios. The researcher will be here soon to give you a form. Hope you have a great day.
