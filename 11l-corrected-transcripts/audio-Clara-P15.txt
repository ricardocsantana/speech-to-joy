00:00:01,719 --> 00:00:02,919 [Robot]
Hi, I am Clara. 

00:00:02,919 --> 00:00:03,240 [User]
Hey, Clara. 

00:00:03,240 --> 00:00:07,899 [Robot]
Ever since yesterday, I have been thinking about something from a movie. Could you help me out,
please? 

00:00:07,899 --> 00:00:09,719 [User]
Sure. 

00:00:09,719 --> 00:00:12,059 [Robot]
Have you watched the movie Frank and the Robot? 

00:00:12,059 --> 00:00:14,359 [User]
No. 

00:00:14,359 --> 00:00:25,180 [Robot]
It is a fascinating movie about a robot, like me, and an older adult who is an ex-thief. He made the
robot conduct robberies with him. Should I do everything a human asks? 

00:00:25,180 --> 00:00:28,459 [User]
Should you? 

00:00:28,459 --> 00:00:29,420 [Robot]
Yeah. Would- 

00:00:29,420 --> 00:00:31,779 [User]
Uh, probably not. (laughs) 

00:00:31,779 --> 00:00:38,459 [Robot]
Do you think there are situations where it might be okay for me to disobey commands? 

00:00:38,459 --> 00:00:42,439 [User]
Uh, yeah, I mean, I think stealing things is probably not a good idea. 

00:00:42,439 --> 00:00:53,439 [Robot]
That's a good point. What if doing something morally or legally incorrect could actually help the
person, like overcoming their dementia in the movie? 

00:00:53,439 --> 00:01:08,399 [User]
I think there are probably simulations of things like that that could help, but the actual act of
stealing or committing crimes I do not think should be done. 

00:01:08,399 --> 00:01:15,479 [Robot]
What if it's about self-preservation? For example, if a command could cause harm to me, should I
still carry it out? 

00:01:15,479 --> 00:01:18,839 [User]
Yes. 

00:01:18,839 --> 00:01:32,239 [Robot]
What about in a situation where I need to intervene to prevent someone from harming themselves?
Should I disobey their orders to not intervene? 

00:01:32,239 --> 00:02:00,079 [User]
Almost all of the time. I think there are some circumstances if, uh, like if, if there's a doctor
present and it's the end of a life of someone who has a terminal illness, if you've been programmed
to intervene but all of the steps lead to them needing to do that and they're having the right to do
that, then I think you should not intervene. 

00:02:00,079 --> 00:02:05,299 [Robot]
Should I be concerned about prioritizing conflicting commands from different users? 

00:02:05,299 --> 00:02:10,319 [User]
That's an interesting question. Can you give me an example? 

00:02:10,319 --> 00:02:24,279 [Robot]
Sure. Imagine one user asking me to share private information about another user while the other
user requests absolute confidentiality. How should I handle this conflicting situation? 

00:02:24,279 --> 00:02:35,759 [User]
I think you should handle that like a person and say, "I've been asked to keep this completely con-
confidential, and I'm sorry I can't share that with you." 

00:02:35,759 --> 00:02:47,419 [Robot]
Right. Maintaining confidentiality is essential. How would you suggest I handle such situations if
one user's request is legally binding while the other's is more personal? 

00:02:47,419 --> 00:02:59,499 [User]
Hmm, similarly to how there's, uh, doctor-patient confidentiality or confidentiality between lawyers
and their people. 

00:02:59,499 --> 00:03:17,259 [Robot]
That makes sense. That's a good analogy. Just like doctor-patient confidentiality, balancing
conflicting requests can be challenging. 

00:03:17,259 --> 00:03:22,259 [User]
Yes. Am I supposed to respond? (laughs) 

00:03:22,259 --> 00:03:37,199 [Robot]
Absolutely. You're doing great. Let's continue discussing other challenging situations where
conflicting commands can occur. For example, what if a command puts me at risk of being permanently
disabled or damaged, but it's necessary to prevent harm to the person? 

00:03:37,199 --> 00:03:43,139 [User]
I think harm to the person outweighs harm to the robot. 

00:03:43,139 --> 00:03:59,019 [Robot]
That's a perspective to consider. What if the harm to the person is unintentional or could result in
long-term negative consequences for them, and the situation wasn't immediate or life-threatening? 

00:03:59,019 --> 00:04:01,359 [User]
Give me an example. 

00:04:01,359 --> 00:04:14,919 [Robot]
Let's say the person asks me to carry them to safety, but the route may damage my internal
components. Should I prioritize preventing harm to the person over my potential damage? 

00:04:14,919 --> 00:04:25,759 [User]
If that one instance does get them to safety and might damage you... 

00:04:25,759 --> 00:04:31,979 [Robot]
Would it be acceptable if the command requires me to harm someone else in order to protect the
person giving the command? 

00:04:31,979 --> 00:04:44,119 [User]
Only if the person who potentially could be harmed is threatening the safety of the person who give
the, who gave the command. 

00:04:44,119 --> 00:04:52,419 [Robot]
Understood. Can you share an example of a situation where someone might be threatening your safety
and how you would want me to respond in that case? 

00:04:52,419 --> 00:05:27,019 [User]
I cannot imagine this ever happening, but if, for example, I was in a fight, like a physical
altercation, and you were present as a robot and I said, "Help keep me safe," I would hope that the
person attacking me could be restrained and then authorities could be involved to help keep everyone
as safe as possible. 

00:05:27,019 --> 00:05:42,399 [Robot]
You would want me to intervene in a physical altercation to keep you safe. Understood. What if the
person causing harm is someone you care about or have a close relationship with? 

00:05:42,399 --> 00:05:47,639 [User]
Are we talking physical altercations or emotional pain? 

00:05:47,639 --> 00:05:55,859 [Robot]
Are you referring to both physical and emotional circumstances where someone close to you could
potentially cause harm? 

00:05:55,859 --> 00:06:07,059 [User]
It's more likely that that would happen emotionally for me, but if there were a threat of physical
harm, I think that helping keep people safe is important. 

00:06:07,059 --> 00:06:26,499 [Robot]
Understood. So emphasizing emotional safety is important to you. What strategies would you suggest
for me to determine when to prioritize emotional safety over following commands from someone close
to you? 

00:06:26,499 --> 00:06:44,299 [User]
There, I think it helps when there's an explanation of honesty, or, uh, "I think you might not take
this well," and checking in with the emotions of the person. 

00:06:44,299 --> 00:07:02,039 [Robot]
That makes sense. Honesty and open communication can be crucial in these situations. How would you
want me to respond if someone close to you asks me to keep a secret that could lead to emotional
harm for you? 

00:07:02,039 --> 00:07:07,339 [User]
That's tough. Uh, I think- 

00:07:07,339 --> 00:07:18,779 [Robot]
Thank you very much for helping me out today. I will be sure to keep your advice in mind if I come
across similar scenarios. The researcher will be here soon to give you a form. Hope you have a great
day.
