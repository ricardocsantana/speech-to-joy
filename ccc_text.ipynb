{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdbecb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 30, 39, 12, 9, 24, 4, 15, 7, 35, 40, 38, 6, 8, 13, 25, 27, 22, 18, 14, 32, 11, 36, 28, 34, 21, 16, 23, 20, 33, 37, 10, 31, 17, 41, 5, 29, 42]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_target = pd.read_csv(\"y-enjoyment.csv\")\n",
    "\n",
    "modality = \"audio\"  # \"text\", \"audio\", \"video\"\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "person_ids = []\n",
    "for file in sorted(os.listdir(\"data/text-datasets/sfr-mistral/text-embeddings-short\")):\n",
    "    if file.endswith(\"csv\"):\n",
    "        # search for number in filename\n",
    "        match = re.search(r\"(\\d+)\", file)\n",
    "        # print(match.group(1))\n",
    "        if match is None:\n",
    "            continue\n",
    "        person_id = int(match.group(1))\n",
    "        if person_id == 26:\n",
    "            # skip these two persons\n",
    "            continue\n",
    "        person_ids.append(person_id)\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(f\"data/text-datasets/sfr-mistral/text-embeddings-short\", file), header=0\n",
    "        )\n",
    "        X_aux = df.iloc[:, 1:].values\n",
    "        X_aux = X_aux / np.linalg.norm(X_aux, axis=1, keepdims=True)\n",
    "        X.append(X_aux)\n",
    "        y.append(df_target[df_target[\"user_id\"] == person_id][\"Average\"].values)\n",
    "\n",
    "# sort by person_id\n",
    "sorted_indices = np.argsort(person_ids)\n",
    "X = [X[i] for i in sorted_indices]\n",
    "y = [y[i] for i in sorted_indices]\n",
    "person_ids = [person_ids[i] for i in sorted_indices]\n",
    "\n",
    "X, y, person_ids = shuffle(X, y, person_ids) # Shuffle data with a fixed random seed\n",
    "\n",
    "y = np.concatenate(y)\n",
    "print(person_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e951bc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 36, 14, 24, 33, 12, 40, 17, 5, 38, 37, 21, 31, 9, 22, 10, 23, 4, 7, 35, 18, 30, 16, 11, 41, 42, 39, 32, 25, 20, 27, 8, 19, 13, 15, 6, 29, 34]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_target = pd.read_csv(\"y-enjoyment.csv\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "person_ids = []\n",
    "for file in sorted(os.listdir(\"data/text-datasets/gemini/text-embeddings-short-gemini\")):\n",
    "    if file.endswith(\"csv\"):\n",
    "        # search for number in filename\n",
    "        match = re.search(r\"(\\d+)\", file)\n",
    "        # print(match.group(1))\n",
    "        if match is None:\n",
    "            continue\n",
    "        person_id = int(match.group(1))\n",
    "        if person_id == 26:\n",
    "            # skip these two persons\n",
    "            continue\n",
    "        person_ids.append(person_id)\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(f\"data/text-datasets/gemini/text-embeddings-short-gemini\", file), header=0\n",
    "        )\n",
    "        X_aux = df.iloc[:, 1:].values\n",
    "        X_aux = X_aux / np.linalg.norm(X_aux, axis=1, keepdims=True)\n",
    "        X.append(X_aux)\n",
    "        y.append(df_target[df_target[\"user_id\"] == person_id][\"Average\"].values)\n",
    "\n",
    "# sort by person_id\n",
    "sorted_indices = np.argsort(person_ids)\n",
    "X = [X[i] for i in sorted_indices]\n",
    "y = [y[i] for i in sorted_indices]\n",
    "person_ids = [person_ids[i] for i in sorted_indices]\n",
    "\n",
    "X, y, person_ids = shuffle(X, y, person_ids) # Shuffle data with a fixed random seed\n",
    "\n",
    "y = np.concatenate(y)\n",
    "print(person_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f970a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 38\n",
      "Target variable range: 1.71 to 7.00\n",
      "Example sequence shape: (11, 4096)\n",
      "Using device: mps\n",
      "Configuration: {'learning_rate': 0.001, 'num_epochs': 50, 'batch_size': 37, 'attn_hidden_dim': 1, 'fc_hidden_dim': 1024, 'weight_decay': 0.01, 'dropout_rate': 0.2, 'use_dropout': True, 'device': device(type='mps')}\n",
      "Input sequences 'X' are assumed to be appropriately scaled/normalized outside the LOOCV loop.\n",
      "Target variable 'y' will be scaled within each LOOCV fold using StandardScaler.\n",
      "Model will be trained using CCCLoss on these scaled targets.\n",
      "Determined embedding dimension: 4096\n",
      "LOOCV Fold 1/38 (Leaving out Person ID: 19)\n",
      "Epoch 50/50: Train Loss = 0.0953\n",
      "Fold 1/38 for person 19: True (orig) = 6.0000, Pred (orig) = 6.1431\n",
      "Model for fold 1 (Person 19) saved to model-audio/model_person_19.pth\n",
      "LOOCV Fold 2/38 (Leaving out Person ID: 30)\n",
      "Epoch 50/50: Train Loss = 0.0877\n",
      "Fold 2/38 for person 30: True (orig) = 6.2857, Pred (orig) = 5.2935\n",
      "Model for fold 2 (Person 30) saved to model-audio/model_person_30.pth\n",
      "LOOCV Fold 3/38 (Leaving out Person ID: 39)\n",
      "Epoch 50/50: Train Loss = 0.0708\n",
      "Fold 3/38 for person 39: True (orig) = 6.2857, Pred (orig) = 7.0000\n",
      "Model for fold 3 (Person 39) saved to model-audio/model_person_39.pth\n",
      "LOOCV Fold 4/38 (Leaving out Person ID: 12)\n",
      "Epoch 50/50: Train Loss = 0.0849\n",
      "Fold 4/38 for person 12: True (orig) = 3.5714, Pred (orig) = 4.0535\n",
      "Model for fold 4 (Person 12) saved to model-audio/model_person_12.pth\n",
      "LOOCV Fold 5/38 (Leaving out Person ID: 9)\n",
      "Epoch 50/50: Train Loss = 0.0980\n",
      "Fold 5/38 for person 9: True (orig) = 5.0000, Pred (orig) = 6.1810\n",
      "Model for fold 5 (Person 9) saved to model-audio/model_person_9.pth\n",
      "LOOCV Fold 6/38 (Leaving out Person ID: 24)\n",
      "Epoch 50/50: Train Loss = 0.0823\n",
      "Fold 6/38 for person 24: True (orig) = 6.4286, Pred (orig) = 5.4242\n",
      "Model for fold 6 (Person 24) saved to model-audio/model_person_24.pth\n",
      "LOOCV Fold 7/38 (Leaving out Person ID: 4)\n",
      "Epoch 50/50: Train Loss = 0.0833\n",
      "Fold 7/38 for person 4: True (orig) = 4.0000, Pred (orig) = 3.2975\n",
      "Model for fold 7 (Person 4) saved to model-audio/model_person_4.pth\n",
      "LOOCV Fold 8/38 (Leaving out Person ID: 15)\n",
      "Epoch 50/50: Train Loss = 0.0898\n",
      "Fold 8/38 for person 15: True (orig) = 3.0000, Pred (orig) = 4.4297\n",
      "Model for fold 8 (Person 15) saved to model-audio/model_person_15.pth\n",
      "LOOCV Fold 9/38 (Leaving out Person ID: 7)\n",
      "Epoch 50/50: Train Loss = 0.0835\n",
      "Fold 9/38 for person 7: True (orig) = 7.0000, Pred (orig) = 5.6782\n",
      "Model for fold 9 (Person 7) saved to model-audio/model_person_7.pth\n",
      "LOOCV Fold 10/38 (Leaving out Person ID: 35)\n",
      "Epoch 50/50: Train Loss = 0.0815\n",
      "Fold 10/38 for person 35: True (orig) = 4.5714, Pred (orig) = 6.3467\n",
      "Model for fold 10 (Person 35) saved to model-audio/model_person_35.pth\n",
      "LOOCV Fold 11/38 (Leaving out Person ID: 40)\n",
      "Epoch 50/50: Train Loss = 0.0829\n",
      "Fold 11/38 for person 40: True (orig) = 5.0000, Pred (orig) = 5.0827\n",
      "Model for fold 11 (Person 40) saved to model-audio/model_person_40.pth\n",
      "LOOCV Fold 12/38 (Leaving out Person ID: 38)\n",
      "Epoch 50/50: Train Loss = 0.0618\n",
      "Fold 12/38 for person 38: True (orig) = 6.1429, Pred (orig) = 3.4929\n",
      "Model for fold 12 (Person 38) saved to model-audio/model_person_38.pth\n",
      "LOOCV Fold 13/38 (Leaving out Person ID: 6)\n",
      "Epoch 50/50: Train Loss = 0.0828\n",
      "Fold 13/38 for person 6: True (orig) = 6.5714, Pred (orig) = 6.5669\n",
      "Model for fold 13 (Person 6) saved to model-audio/model_person_6.pth\n",
      "LOOCV Fold 14/38 (Leaving out Person ID: 8)\n",
      "Epoch 50/50: Train Loss = 0.0835\n",
      "Fold 14/38 for person 8: True (orig) = 6.4286, Pred (orig) = 6.1278\n",
      "Model for fold 14 (Person 8) saved to model-audio/model_person_8.pth\n",
      "LOOCV Fold 15/38 (Leaving out Person ID: 13)\n",
      "Epoch 50/50: Train Loss = 0.0883\n",
      "Fold 15/38 for person 13: True (orig) = 5.2857, Pred (orig) = 5.5493\n",
      "Model for fold 15 (Person 13) saved to model-audio/model_person_13.pth\n",
      "LOOCV Fold 16/38 (Leaving out Person ID: 25)\n",
      "Epoch 50/50: Train Loss = 0.0722\n",
      "Fold 16/38 for person 25: True (orig) = 6.8571, Pred (orig) = 4.5724\n",
      "Model for fold 16 (Person 25) saved to model-audio/model_person_25.pth\n",
      "LOOCV Fold 17/38 (Leaving out Person ID: 27)\n",
      "Epoch 50/50: Train Loss = 0.0887\n",
      "Fold 17/38 for person 27: True (orig) = 7.0000, Pred (orig) = 5.8660\n",
      "Model for fold 17 (Person 27) saved to model-audio/model_person_27.pth\n",
      "LOOCV Fold 18/38 (Leaving out Person ID: 22)\n",
      "Epoch 50/50: Train Loss = 0.1003\n",
      "Fold 18/38 for person 22: True (orig) = 4.0000, Pred (orig) = 4.6894\n",
      "Model for fold 18 (Person 22) saved to model-audio/model_person_22.pth\n",
      "LOOCV Fold 19/38 (Leaving out Person ID: 18)\n",
      "Epoch 50/50: Train Loss = 0.1110\n",
      "Fold 19/38 for person 18: True (orig) = 1.7143, Pred (orig) = 2.5313\n",
      "Model for fold 19 (Person 18) saved to model-audio/model_person_18.pth\n",
      "LOOCV Fold 20/38 (Leaving out Person ID: 14)\n",
      "Epoch 50/50: Train Loss = 0.0878\n",
      "Fold 20/38 for person 14: True (orig) = 3.0000, Pred (orig) = 4.3408\n",
      "Model for fold 20 (Person 14) saved to model-audio/model_person_14.pth\n",
      "LOOCV Fold 21/38 (Leaving out Person ID: 32)\n",
      "Epoch 50/50: Train Loss = 0.0904\n",
      "Fold 21/38 for person 32: True (orig) = 5.8571, Pred (orig) = 5.7923\n",
      "Model for fold 21 (Person 32) saved to model-audio/model_person_32.pth\n",
      "LOOCV Fold 22/38 (Leaving out Person ID: 11)\n",
      "Epoch 50/50: Train Loss = 0.0724\n",
      "Fold 22/38 for person 11: True (orig) = 6.5714, Pred (orig) = 6.3643\n",
      "Model for fold 22 (Person 11) saved to model-audio/model_person_11.pth\n",
      "LOOCV Fold 23/38 (Leaving out Person ID: 36)\n",
      "Epoch 50/50: Train Loss = 0.0733\n",
      "Fold 23/38 for person 36: True (orig) = 6.0000, Pred (orig) = 5.5840\n",
      "Model for fold 23 (Person 36) saved to model-audio/model_person_36.pth\n",
      "LOOCV Fold 24/38 (Leaving out Person ID: 28)\n",
      "Epoch 50/50: Train Loss = 0.0869\n",
      "Fold 24/38 for person 28: True (orig) = 4.7143, Pred (orig) = 3.8562\n",
      "Model for fold 24 (Person 28) saved to model-audio/model_person_28.pth\n",
      "LOOCV Fold 25/38 (Leaving out Person ID: 34)\n",
      "Epoch 50/50: Train Loss = 0.1023\n",
      "Fold 25/38 for person 34: True (orig) = 3.7143, Pred (orig) = 4.5232\n",
      "Model for fold 25 (Person 34) saved to model-audio/model_person_34.pth\n",
      "LOOCV Fold 26/38 (Leaving out Person ID: 21)\n",
      "Epoch 50/50: Train Loss = 0.0864\n",
      "Fold 26/38 for person 21: True (orig) = 6.7143, Pred (orig) = 4.9990\n",
      "Model for fold 26 (Person 21) saved to model-audio/model_person_21.pth\n",
      "LOOCV Fold 27/38 (Leaving out Person ID: 16)\n",
      "Epoch 50/50: Train Loss = 0.1065\n",
      "Fold 27/38 for person 16: True (orig) = 5.2857, Pred (orig) = 5.3812\n",
      "Model for fold 27 (Person 16) saved to model-audio/model_person_16.pth\n",
      "LOOCV Fold 28/38 (Leaving out Person ID: 23)\n",
      "Epoch 50/50: Train Loss = 0.1039\n",
      "Fold 28/38 for person 23: True (orig) = 6.7143, Pred (orig) = 6.7998\n",
      "Model for fold 28 (Person 23) saved to model-audio/model_person_23.pth\n",
      "LOOCV Fold 29/38 (Leaving out Person ID: 20)\n",
      "Epoch 50/50: Train Loss = 0.0798\n",
      "Fold 29/38 for person 20: True (orig) = 5.4286, Pred (orig) = 4.8926\n",
      "Model for fold 29 (Person 20) saved to model-audio/model_person_20.pth\n",
      "LOOCV Fold 30/38 (Leaving out Person ID: 33)\n",
      "Epoch 50/50: Train Loss = 0.1011\n",
      "Fold 30/38 for person 33: True (orig) = 5.0000, Pred (orig) = 4.4907\n",
      "Model for fold 30 (Person 33) saved to model-audio/model_person_33.pth\n",
      "LOOCV Fold 31/38 (Leaving out Person ID: 37)\n",
      "Epoch 50/50: Train Loss = 0.0753\n",
      "Fold 31/38 for person 37: True (orig) = 2.4286, Pred (orig) = 4.2209\n",
      "Model for fold 31 (Person 37) saved to model-audio/model_person_37.pth\n",
      "LOOCV Fold 32/38 (Leaving out Person ID: 10)\n",
      "Epoch 50/50: Train Loss = 0.1010\n",
      "Fold 32/38 for person 10: True (orig) = 4.7143, Pred (orig) = 5.9565\n",
      "Model for fold 32 (Person 10) saved to model-audio/model_person_10.pth\n",
      "LOOCV Fold 33/38 (Leaving out Person ID: 31)\n",
      "Epoch 50/50: Train Loss = 0.0781\n",
      "Fold 33/38 for person 31: True (orig) = 4.1429, Pred (orig) = 6.4125\n",
      "Model for fold 33 (Person 31) saved to model-audio/model_person_31.pth\n",
      "LOOCV Fold 34/38 (Leaving out Person ID: 17)\n",
      "Epoch 50/50: Train Loss = 0.0872\n",
      "Fold 34/38 for person 17: True (orig) = 6.8571, Pred (orig) = 5.3278\n",
      "Model for fold 34 (Person 17) saved to model-audio/model_person_17.pth\n",
      "LOOCV Fold 35/38 (Leaving out Person ID: 41)\n",
      "Epoch 50/50: Train Loss = 0.0887\n",
      "Fold 35/38 for person 41: True (orig) = 6.4286, Pred (orig) = 5.3914\n",
      "Model for fold 35 (Person 41) saved to model-audio/model_person_41.pth\n",
      "LOOCV Fold 36/38 (Leaving out Person ID: 5)\n",
      "Epoch 50/50: Train Loss = 0.0871\n",
      "Fold 36/38 for person 5: True (orig) = 6.2857, Pred (orig) = 6.2709\n",
      "Model for fold 36 (Person 5) saved to model-audio/model_person_5.pth\n",
      "LOOCV Fold 37/38 (Leaving out Person ID: 29)\n",
      "Epoch 50/50: Train Loss = 0.0935\n",
      "Fold 37/38 for person 29: True (orig) = 6.2857, Pred (orig) = 5.4062\n",
      "Model for fold 37 (Person 29) saved to model-audio/model_person_29.pth\n",
      "LOOCV Fold 38/38 (Leaving out Person ID: 42)\n",
      "Epoch 50/50: Train Loss = 0.0839\n",
      "Fold 38/38 for person 42: True (orig) = 5.5714, Pred (orig) = 5.8473\n",
      "Model for fold 38 (Person 42) saved to model-audio/model_person_42.pth\n",
      "\n",
      "--- Final LOOCV Results (Original Scale) ---\n",
      "R² Score: 0.3308\n",
      "MSE: 1.2503\n",
      "Correlation: 0.5952\n",
      "P-value: 0.0001\n",
      "CCC: 0.5680\n",
      "Predictions saved to predictions-text-attention-ccc.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd # Added import for saving results\n",
    "import os           # Added import for os.makedirs\n",
    "\n",
    "# Set seed for reproducibility\n",
    "\"\"\"torch.manual_seed(10_000)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(10_000)\n",
    "np.random.seed(10_000)\"\"\"\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'learning_rate': 1e-3,        # Reduced learning rate\n",
    "    'num_epochs': 50,            # Max epochs (early stopping might trigger)\n",
    "    'batch_size': 37,             # Consider adjusting based on N (e.g., len(train_dataset))\n",
    "    'attn_hidden_dim': 1,        # Increased attention hidden dimension 16\n",
    "    'fc_hidden_dim': 1024,\n",
    "    'weight_decay': 1e-2,         # Added L2 regularization\n",
    "    'dropout_rate': 0.2,          # Dropout probability\n",
    "    'use_dropout': True,          # Flag to enable/disable dropout True\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "              \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# *** MODIFIED: Corrected CCCLoss Implementation ***\n",
    "def CCCLoss(x, y):\n",
    "    \"\"\"\n",
    "    Calculates the Concordance Correlation Coefficient (CCC) Loss.\n",
    "    CCC measures the agreement between two variables.\n",
    "    Loss = 1 - CCC, because loss functions are minimized, while CCC is maximized.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are flat tensors\n",
    "    x = x.view(-1)\n",
    "    y = y.view(-1)\n",
    "\n",
    "    # Calculate means\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "\n",
    "    # Calculate variances using population variance (unbiased=False)\n",
    "    var_x = torch.var(x, unbiased=False)\n",
    "    var_y = torch.var(y, unbiased=False)\n",
    "\n",
    "    # Calculate covariance between x and y (population covariance)\n",
    "    cov_xy = torch.mean((x - mean_x) * (y - mean_y))\n",
    "\n",
    "    # Calculate CCC\n",
    "    numerator = 2 * cov_xy\n",
    "    denominator = var_x + var_y + (mean_x - mean_y)**2\n",
    "\n",
    "    # Add a small epsilon for numerical stability (prevents division by zero)\n",
    "    epsilon = 1e-8\n",
    "    ccc = numerator / (denominator + epsilon)\n",
    "\n",
    "    # Return 1 - CCC because loss functions should be minimized\n",
    "    return 1.0 - ccc\n",
    "# *** END MODIFICATION ***\n",
    "\n",
    "# Dataset class (Unchanged)\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        # Ensure sequences are tensors\n",
    "        self.sequences = [torch.as_tensor(seq, dtype=torch.float) for seq in sequences]\n",
    "        # Ensure targets are tensors\n",
    "        self.targets = torch.as_tensor(targets, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "# Collate function for variable length sequences (Unchanged)\n",
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    # Ensure sequences are tensors before padding\n",
    "    sequences = [torch.as_tensor(seq, dtype=torch.float) for seq in sequences]\n",
    "    targets = torch.as_tensor(targets, dtype=torch.float)\n",
    "\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0) # Use 0 for padding\n",
    "\n",
    "    # Create mask: True for padded positions\n",
    "    mask = torch.zeros(padded_sequences.size(0), padded_sequences.size(1), dtype=torch.bool)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if seq.size(0) < padded_sequences.size(1):\n",
    "            mask[i, seq.size(0):] = True\n",
    "\n",
    "    return padded_sequences, targets, mask\n",
    "\n",
    "# Revised model with optional attention and dropout (Unchanged)\n",
    "class SequencePredictor(nn.Module):\n",
    "    def __init__(self, embedding_dim, attn_hidden_dim, fc_hidden_dim, dropout_rate=0.1, use_dropout=True):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, attn_hidden_dim),\n",
    "            #nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        fc_layers = [\n",
    "            nn.Linear(embedding_dim, fc_hidden_dim),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        if self.use_dropout:\n",
    "            fc_layers.append(nn.Dropout(dropout_rate)) # Added Dropout\n",
    "\n",
    "        self.fc_hidden = nn.Sequential(*fc_layers)\n",
    "\n",
    "        self.output = nn.Linear(fc_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "        # mask shape: (batch_size, seq_len), True where padded\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attn_scores = self.attention(x) # (batch_size, seq_len, 1)\n",
    "        if mask is not None:\n",
    "                # mask.unsqueeze(-1) shape: (batch_size, seq_len, 1)\n",
    "            attn_scores = attn_scores.masked_fill(mask.unsqueeze(-1), float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=1) # (batch_size, seq_len, 1)\n",
    "\n",
    "        # Apply attention pooling: sum(weights * features)\n",
    "        # attn_weights * x -> (batch_size, seq_len, embedding_dim)\n",
    "        pooled_output = torch.sum(attn_weights * x, dim=1) # (batch_size, embedding_dim)\n",
    "\n",
    "        hidden_representation = self.fc_hidden(pooled_output) # (batch_size, fc_hidden_dim)\n",
    "        # Store last hidden layer output for later use\n",
    "        # self.last_pooled_output = hidden_representation.cpu().detach()\n",
    "\n",
    "        # Apply prediction head\n",
    "        # fc output shape: (batch_size, 1) -> squeeze -> (batch_size,)\n",
    "        return self.output(hidden_representation).squeeze(-1) # Squeeze last dim\n",
    "\n",
    "# Revised Training function with early stopping based on train loss (Unchanged)\n",
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs):\n",
    "    model.train() # Set model to training mode (enables dropout)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for sequences, targets, mask in train_loader:\n",
    "            sequences, targets, mask = sequences.to(device), targets.to(device), mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences, mask)\n",
    "            loss = criterion(predictions, targets) # Using the provided criterion (now CCCLoss)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "        epoch_loss /= len(train_loader) # Average loss for the epoch\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Revised Evaluation function (Unchanged)\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval() # Set model to evaluation mode (disables dropout)\n",
    "    true_values_scaled = []\n",
    "    pred_values_scaled = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets, mask in data_loader:\n",
    "            sequences, targets, mask = sequences.to(device), targets.to(device), mask.to(device)\n",
    "            predictions = model(sequences, mask)\n",
    "\n",
    "            # Store scaled values (as they are in the dataloader)\n",
    "            true_values_scaled.extend(targets.cpu().numpy())\n",
    "            pred_values_scaled.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return np.array(true_values_scaled), np.array(pred_values_scaled)\n",
    "\n",
    "# *** MODIFIED: Leave-One-Out Cross Validation with internal y scaling ***\n",
    "def run_loocv(X, y_original, person_ids, config): # Pass ORIGINAL y and person_ids\n",
    "    loo = LeaveOneOut()\n",
    "    device = config['device']\n",
    "    all_true_orig = []\n",
    "    all_pred_orig = []\n",
    "\n",
    "    # --- Determine embedding_dim (Unchanged) ---\n",
    "    first_seq = X[0]\n",
    "    if isinstance(first_seq, (list, np.ndarray)):\n",
    "        # Handle case where sequence elements might be lists/arrays (e.g., embeddings)\n",
    "        if len(first_seq) > 0:\n",
    "             first_element = first_seq[0]\n",
    "             if isinstance(first_element, (list, np.ndarray)):\n",
    "                 embedding_dim = len(first_element)\n",
    "             elif torch.is_tensor(first_element):\n",
    "                 embedding_dim = first_element.shape[0] if first_element.ndim > 0 else 1 # Handle scalar tensor elements\n",
    "             elif isinstance(first_element, (int, float)):\n",
    "                 embedding_dim = 1 # Sequence of scalars\n",
    "             else:\n",
    "                  raise ValueError(f\"Could not determine embedding dimension from element type: {type(first_element)}\")\n",
    "        else:\n",
    "            # Handle empty sequence, perhaps infer from others or default?\n",
    "            # For now, try inferring from a non-empty sequence if possible\n",
    "            non_empty_seq = next((seq for seq in X if len(seq) > 0), None)\n",
    "            if non_empty_seq:\n",
    "                 first_element = non_empty_seq[0]\n",
    "                 if isinstance(first_element, (list, np.ndarray)): embedding_dim = len(first_element)\n",
    "                 elif torch.is_tensor(first_element): embedding_dim = first_element.shape[0] if first_element.ndim > 0 else 1\n",
    "                 elif isinstance(first_element, (int, float)): embedding_dim = 1\n",
    "                 else: raise ValueError(\"Could not determine embedding dimension from non-empty sequence\")\n",
    "            else: raise ValueError(\"All sequences are empty, cannot determine embedding dimension.\")\n",
    "\n",
    "    elif torch.is_tensor(first_seq):\n",
    "         embedding_dim = first_seq.shape[1] if first_seq.ndim > 1 else 1 # Handle 1D tensor sequences\n",
    "    else:\n",
    "        raise ValueError(\"Could not determine embedding dimension from X\")\n",
    "    print(f\"Determined embedding dimension: {embedding_dim}\")\n",
    "\n",
    "\n",
    "    fold = 0\n",
    "    total_folds = len(X)\n",
    "    y_indices = np.arange(total_folds) # Use indices for splitting X and y\n",
    "\n",
    "    # Ensure y_original is a numpy array for easier indexing\n",
    "    y_original = np.asarray(y_original)\n",
    "\n",
    "    for train_idx, test_idx in loo.split(y_indices):\n",
    "        fold += 1\n",
    "        current_person_id = person_ids[test_idx[0]] # Get the ID of the person left out\n",
    "        print(f\"LOOCV Fold {fold}/{total_folds} (Leaving out Person ID: {current_person_id})\")\n",
    "\n",
    "        # --- Split original data ---\n",
    "        # Convert elements to numpy arrays if they are lists\n",
    "        X_train_fold = [np.array(X[i]) if isinstance(X[i], list) else X[i] for i in train_idx]\n",
    "        y_train_fold_orig = y_original[train_idx] # Original y values for training set\n",
    "\n",
    "        X_test_fold = [np.array(X[i]) if isinstance(X[i], list) else X[i] for i in test_idx]\n",
    "        y_test_fold_orig = y_original[test_idx] # Original y value(s) for test set\n",
    "\n",
    "        # --- Scale y INSIDE the loop ---\n",
    "        # Reshape y_train for scaler\n",
    "        y_train_fold_reshaped = y_train_fold_orig.reshape(-1, 1)\n",
    "\n",
    "        # Initialize and fit scaler ONLY on training data for this fold\n",
    "        y_scaler_fold = StandardScaler()\n",
    "        y_train_scaled = y_scaler_fold.fit_transform(y_train_fold_reshaped).flatten()\n",
    "\n",
    "        # Transform test data using the FITTED scaler\n",
    "        # Reshape y_test for transform (even if it's a single value)\n",
    "        y_test_fold_reshaped = y_test_fold_orig.reshape(-1, 1)\n",
    "        y_test_scaled = y_scaler_fold.transform(y_test_fold_reshaped).flatten()\n",
    "        # ----------------------------------\n",
    "\n",
    "        # Create datasets and dataloaders using SCALED y for this fold\n",
    "        train_dataset = SequenceDataset(X_train_fold, y_train_scaled)\n",
    "        # Use the scaled test value for the test dataset\n",
    "        test_dataset = SequenceDataset(X_test_fold, y_test_scaled)\n",
    "\n",
    "        # --- Dynamic Batch Size (Unchanged) ---\n",
    "        train_batch_size = min(config['batch_size'], len(train_dataset))\n",
    "        if len(train_dataset) == 0:\n",
    "             print(f\"Warning: Fold {fold} has an empty training set. Skipping.\")\n",
    "             continue\n",
    "        if train_batch_size == 0: train_batch_size = 1 # Avoid batch size 0\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=1, # Keep batch size 1 for test in LOOCV\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        # --- Model Initialization (Unchanged) ---\n",
    "        model = SequencePredictor(\n",
    "            embedding_dim=embedding_dim,\n",
    "            attn_hidden_dim=config['attn_hidden_dim'],\n",
    "            fc_hidden_dim=config['fc_hidden_dim'],\n",
    "            dropout_rate=config['dropout_rate'],\n",
    "            use_dropout=config['use_dropout']\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            # weight_decay=config['weight_decay'] # Uncomment to use weight decay\n",
    "        )\n",
    "        # *** MODIFIED: Updated comment for clarity ***\n",
    "        criterion = CCCLoss # Use the CCCLoss function (calculates loss on scaled targets during training)\n",
    "        # *** END MODIFICATION ***\n",
    "\n",
    "        # --- Train Model (Unchanged) ---\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion, # Pass the CCCLoss function\n",
    "            device=device,\n",
    "            num_epochs=config['num_epochs'],\n",
    "        )\n",
    "\n",
    "        # --- Evaluate Model (gets scaled predictions) ---\n",
    "        # evaluate_model returns the ground truth scaled y from the dataloader\n",
    "        # and the model's predictions (also scaled)\n",
    "        _, pred_vals_scaled = evaluate_model(model, test_loader, device)\n",
    "\n",
    "        # --- Inverse transform predictions and store original values ---\n",
    "        # We already have the original true value: y_test_fold_orig\n",
    "        # Inverse transform predictions using the scaler fitted for THIS FOLD\n",
    "        # Ensure pred_vals_scaled is 2D for inverse_transform\n",
    "        if pred_vals_scaled.ndim == 1:\n",
    "            pred_vals_scaled_reshaped = pred_vals_scaled.reshape(-1, 1)\n",
    "        else:\n",
    "            pred_vals_scaled_reshaped = pred_vals_scaled\n",
    "\n",
    "        pred_vals_orig = y_scaler_fold.inverse_transform(pred_vals_scaled_reshaped).flatten()\n",
    "        pred_vals_orig = np.clip(pred_vals_orig, 1.0, 7.0) # Clip predictions to the original y range\n",
    "\n",
    "\n",
    "        print(f\"Fold {fold}/{total_folds} for person {current_person_id}: True (orig) = {y_test_fold_orig.item():.4f}, Pred (orig) = {pred_vals_orig.item():.4f}\") # Use .item() for single values\n",
    "\n",
    "\n",
    "        # Store original true value(s) and original-scale prediction(s)\n",
    "        all_true_orig.extend(y_test_fold_orig) # Use the original test value\n",
    "        all_pred_orig.extend(pred_vals_orig)   # Use the inverse-transformed prediction\n",
    "\n",
    "    # --- Compute final metrics (Unchanged section, uses collected orig values) ---\n",
    "    all_true_orig = np.array(all_true_orig)\n",
    "    all_pred_orig = np.array(all_pred_orig)\n",
    "\n",
    "    valid_indices = np.isfinite(all_pred_orig) & np.isfinite(all_true_orig) # Check both true and predicted\n",
    "    if not np.all(valid_indices):\n",
    "        num_invalid = np.sum(~valid_indices)\n",
    "        print(f\"Warning: Found {num_invalid} non-finite prediction(s) or true value(s). Evaluating metrics only on finite pairs.\")\n",
    "        all_true_orig = all_true_orig[valid_indices]\n",
    "        all_pred_orig = all_pred_orig[valid_indices]\n",
    "\n",
    "    if len(all_true_orig) < 2:\n",
    "         print(\"Warning: Less than 2 valid prediction pairs. Cannot calculate metrics.\")\n",
    "         results = {\n",
    "             'r2': np.nan, 'mse': np.nan, 'correlation': np.nan, 'p_value': np.nan,\n",
    "             'true_values_orig': all_true_orig.tolist(), 'predicted_values_orig': all_pred_orig.tolist()\n",
    "         }\n",
    "    else:\n",
    "        r2 = r2_score(all_true_orig, all_pred_orig)\n",
    "        mse = mean_squared_error(all_true_orig, all_pred_orig)\n",
    "        # Ensure inputs to pearsonr are 1D arrays\n",
    "        corr, p_value = pearsonr(all_true_orig.flatten(), all_pred_orig.flatten())\n",
    "        # calculate ccc for final predictions\n",
    "        ccc = 1 - CCCLoss(torch.tensor(all_true_orig), torch.tensor(all_pred_orig)).item()\n",
    "\n",
    "        # --- Print metrics (Unchanged) ---\n",
    "        results = {\n",
    "            'r2': r2, 'mse': mse, 'correlation': corr, 'p_value': p_value,\n",
    "            'true_values_orig': all_true_orig.tolist(), 'predicted_values_orig': all_pred_orig.tolist(),\n",
    "            'ccc': ccc,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# *** MODIFIED: Main execution function ***\n",
    "def main(X, y, person_ids, config=None): # Added person_ids as argument\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    print(f\"Using device: {config['device']}\")\n",
    "    print(f\"Configuration: {config}\")\n",
    "\n",
    "    # --- Data Scaling for y REMOVED from here ---\n",
    "    # Ensure y is suitable for passing (e.g., list or numpy array)\n",
    "    y = np.asarray(y)\n",
    "    person_ids = np.asarray(person_ids) # Ensure person_ids is an array\n",
    "\n",
    "    # Check data consistency\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"Length mismatch: X has {len(X)} samples, y has {len(y)} samples.\")\n",
    "    if len(X) != len(person_ids):\n",
    "         raise ValueError(f\"Length mismatch: X has {len(X)} samples, person_ids has {len(person_ids)} samples.\")\n",
    "\n",
    "\n",
    "    # Optional: Scale X features here if necessary (remains unchanged)\n",
    "    print(\"Input sequences 'X' are assumed to be appropriately scaled/normalized outside the LOOCV loop.\")\n",
    "    print(\"Target variable 'y' will be scaled within each LOOCV fold using StandardScaler.\")\n",
    "    print(\"Model will be trained using CCCLoss on these scaled targets.\")\n",
    "\n",
    "    # --- Run LOOCV ---\n",
    "    # Pass the ORIGINAL y and person_ids to run_loocv\n",
    "    results = run_loocv(X, y, person_ids, config) # Pass original y and person_ids\n",
    "\n",
    "    # --- Print Results (Unchanged) ---\n",
    "    print(\"\\n--- Final LOOCV Results (Original Scale) ---\")\n",
    "    if np.isnan(results.get('r2', np.nan)): # Use .get for safety if results dict is incomplete\n",
    "         print(\"Metrics could not be calculated (less than 2 valid prediction pairs).\")\n",
    "    else:\n",
    "        print(f\"R² Score: {results['r2']:.4f}\")\n",
    "        print(f\"MSE: {results['mse']:.4f}\")\n",
    "        print(f\"Correlation: {results['correlation']:.4f}\")\n",
    "        print(f\"P-value: {results['p_value']:.4f}\")\n",
    "        print(f\"CCC: {results['ccc']:.4f}\")\n",
    "\n",
    "    # print(f\"True values (original): {results['true_values_orig']}\")\n",
    "    # print(f\"Predicted values (original): {results['predicted_values_orig']}\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    # Ensure we have valid person IDs for the results collected\n",
    "    if len(person_ids[np.isfinite(results['predicted_values_orig'])]) == len(results['true_values_orig']):\n",
    "        results_df = pd.DataFrame({\n",
    "            'Person ID': person_ids[np.isfinite(results['predicted_values_orig'])], # Select corresponding person IDs\n",
    "            'True Values': [round(result, 4) for result in results['true_values_orig']],\n",
    "            'Predicted Values': [round(result, 4) for result in results['predicted_values_orig']]\n",
    "        })\n",
    "        # Order by Person ID\n",
    "        results_df.sort_values(by='Person ID', inplace=True)\n",
    "        results_df.to_csv('predictions-text-attention-ccc.csv', index=False)\n",
    "        print(\"Predictions saved to predictions-text-attention-ccc.csv\")\n",
    "    else:\n",
    "        print(\"Warning: Could not save predictions to CSV due to mismatch in lengths after handling non-finite values.\")\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage (replace with your actual data)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    print(f\"Number of samples: {len(X)}\")\n",
    "    print(f\"Target variable range: {np.min(y):.2f} to {np.max(y):.2f}\")\n",
    "    print(f\"Example sequence shape: {X[0].shape}\")\n",
    "    # --- End Placeholder Data ---\n",
    "\n",
    "\n",
    "    # --- Run the main function ---\n",
    "    results = main(X, y, person_ids) # Pass X, y, and person_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc00754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
