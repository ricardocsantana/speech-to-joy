# Cite us

@inproceedings{10.1145/3716553.3750747,
author = {Santana, Ricardo and Irfan, Bahar and Lagerstedt, Erik and Skantze, Gabriel and Pereira, Andre},
title = {Speech-to-Joy: Self-Supervised Features for Enjoyment Prediction in Human–Robot Conversation},
year = {2025},
isbn = {9798400714993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {<https://doi.org/10.1145/3716553.3750747}>,
doi = {10.1145/3716553.3750747},
abstract = {Conversational systems that interact or collaborate with people must understand not only task success but also the quality of human experience. We present Speech-to-Joy, a lightweight framework that learns to predict users’ own post-interaction enjoyment ratings using latent embeddings from audio and text modalities. Evaluated on a corpus of human-robot dialogues, the model’s predicted enjoyment correlates strongly and significantly with user self-reports, outperforming both an experienced HRI annotator and heavier LLM-based uni- and multimodal baselines. Notably, even the unimodal audio branch - using only frozen speech embeddings - surpasses all baselines, and a late-fusion of text and audio achieves the highest performance. Designed for real-time inference on resource-limited platforms, Speech-to-Joy replaces ad-hoc emotion heuristics with a direct and user-centered measure of enjoyment. This work paves the way for optimizing interactions with robots and other conversational systems through the lens that matters most: the user’s own experience. 1},
booktitle = {Proceedings of the 27th International Conference on Multimodal Interaction},
pages = {238–248},
numpages = {11},
keywords = {Human-Robot Interaction, Affective Computing, Human-Centered AI, Transfer Learning},
location = {
},
series = {ICMI '25}
}
