{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c7e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 38, 29, 27, 39, 42, 30, 40, 13, 11, 34, 12, 7, 25, 32, 33, 35, 8, 41, 18, 23, 16, 9, 6, 24, 17, 15, 19, 36, 22, 37, 21, 28, 14, 31, 20, 10]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_target = pd.read_csv(\"y-enjoyment.csv\")\n",
    "\n",
    "modality = \"audio\"  # \"text\", \"audio\", \"video\"\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "person_ids = []\n",
    "for file in sorted(os.listdir(\"text-embeddings-short\")):\n",
    "    if file.endswith(\"csv\"):\n",
    "        # search for number in filename\n",
    "        match = re.search(r\"(\\d+)\", file)\n",
    "        # print(match.group(1))\n",
    "        if match is None:\n",
    "            continue\n",
    "        person_id = int(match.group(1))\n",
    "        if person_id == 26:\n",
    "            # skip these two persons\n",
    "            continue\n",
    "        person_ids.append(person_id)\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(f\"text-embeddings-short\", file), header=0\n",
    "        )\n",
    "        X_aux = df.iloc[:, 1:].values\n",
    "        X_aux = X_aux / np.linalg.norm(X_aux, axis=1, keepdims=True)\n",
    "        X.append(X_aux)\n",
    "        y.append(df_target[df_target[\"user_id\"] == person_id][\"Average\"].values)\n",
    "\n",
    "# sort by person_id\n",
    "sorted_indices = np.argsort(person_ids)\n",
    "X = [X[i] for i in sorted_indices]\n",
    "y = [y[i] for i in sorted_indices]\n",
    "person_ids = [person_ids[i] for i in sorted_indices]\n",
    "\n",
    "X, y, person_ids = shuffle(X, y, person_ids) # Shuffle data with a fixed random seed\n",
    "\n",
    "y = np.concatenate(y)\n",
    "print(person_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4416787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Configuration: {'learning_rate': 0.001, 'num_epochs': 120, 'batch_size': 37, 'attn_hidden_dim': 1, 'fc_hidden_dim': 2048, 'weight_decay': 0.01, 'dropout_rate': 0.4, 'use_dropout': True, 'device': device(type='mps')}\n",
      "Input sequences 'X' are assumed to be appropriately scaled/normalized outside the LOOCV loop.\n",
      "Target variable 'y' will be scaled within each LOOCV fold.\n",
      "LOOCV Fold 1/38\n",
      "Epoch 50/120: Train Loss = 0.1513\n",
      "Epoch 100/120: Train Loss = 0.0255\n",
      "Fold 1/38 for person 4: True (orig) = [4.], Pred (orig) = [3.3028705]\n",
      "Model for fold 1 saved to model-audio/model_person_4.pth\n",
      "LOOCV Fold 2/38\n",
      "Epoch 50/120: Train Loss = 0.1224\n",
      "Epoch 100/120: Train Loss = 0.0218\n",
      "Fold 2/38 for person 5: True (orig) = [6.28571429], Pred (orig) = [5.7674794]\n",
      "Model for fold 2 saved to model-audio/model_person_5.pth\n",
      "LOOCV Fold 3/38\n",
      "Epoch 50/120: Train Loss = 0.1022\n",
      "Epoch 100/120: Train Loss = 0.0239\n",
      "Fold 3/38 for person 38: True (orig) = [6.14285714], Pred (orig) = [3.6125236]\n",
      "Model for fold 3 saved to model-audio/model_person_38.pth\n",
      "LOOCV Fold 4/38\n",
      "Epoch 50/120: Train Loss = 0.1181\n",
      "Epoch 100/120: Train Loss = 0.0164\n",
      "Fold 4/38 for person 29: True (orig) = [6.28571429], Pred (orig) = [5.4021907]\n",
      "Model for fold 4 saved to model-audio/model_person_29.pth\n",
      "LOOCV Fold 5/38\n",
      "Epoch 50/120: Train Loss = 0.1696\n",
      "Epoch 100/120: Train Loss = 0.0195\n",
      "Fold 5/38 for person 27: True (orig) = [7.], Pred (orig) = [6.5671263]\n",
      "Model for fold 5 saved to model-audio/model_person_27.pth\n",
      "LOOCV Fold 6/38\n",
      "Epoch 50/120: Train Loss = 0.0903\n",
      "Epoch 100/120: Train Loss = 0.0265\n",
      "Fold 6/38 for person 39: True (orig) = [6.28571429], Pred (orig) = [7.048198]\n",
      "Model for fold 6 saved to model-audio/model_person_39.pth\n",
      "LOOCV Fold 7/38\n",
      "Epoch 50/120: Train Loss = 0.1412\n",
      "Epoch 100/120: Train Loss = 0.0283\n",
      "Fold 7/38 for person 42: True (orig) = [5.57142857], Pred (orig) = [5.676525]\n",
      "Model for fold 7 saved to model-audio/model_person_42.pth\n",
      "LOOCV Fold 8/38\n",
      "Epoch 50/120: Train Loss = 0.0909\n",
      "Epoch 100/120: Train Loss = 0.0351\n",
      "Fold 8/38 for person 30: True (orig) = [6.28571429], Pred (orig) = [4.9471817]\n",
      "Model for fold 8 saved to model-audio/model_person_30.pth\n",
      "LOOCV Fold 9/38\n",
      "Epoch 50/120: Train Loss = 0.0973\n",
      "Epoch 100/120: Train Loss = 0.0291\n",
      "Fold 9/38 for person 40: True (orig) = [5.], Pred (orig) = [4.4211016]\n",
      "Model for fold 9 saved to model-audio/model_person_40.pth\n",
      "LOOCV Fold 10/38\n",
      "Epoch 50/120: Train Loss = 0.1283\n",
      "Epoch 100/120: Train Loss = 0.0257\n",
      "Fold 10/38 for person 13: True (orig) = [5.28571429], Pred (orig) = [5.815719]\n",
      "Model for fold 10 saved to model-audio/model_person_13.pth\n",
      "LOOCV Fold 11/38\n",
      "Epoch 50/120: Train Loss = 0.1207\n",
      "Epoch 100/120: Train Loss = 0.0290\n",
      "Fold 11/38 for person 11: True (orig) = [6.57142857], Pred (orig) = [5.9372487]\n",
      "Model for fold 11 saved to model-audio/model_person_11.pth\n",
      "LOOCV Fold 12/38\n",
      "Epoch 50/120: Train Loss = 0.0953\n",
      "Epoch 100/120: Train Loss = 0.0351\n",
      "Fold 12/38 for person 34: True (orig) = [3.71428571], Pred (orig) = [4.0274262]\n",
      "Model for fold 12 saved to model-audio/model_person_34.pth\n",
      "LOOCV Fold 13/38\n",
      "Epoch 50/120: Train Loss = 0.1150\n",
      "Epoch 100/120: Train Loss = 0.0170\n",
      "Fold 13/38 for person 12: True (orig) = [3.57142857], Pred (orig) = [4.04833]\n",
      "Model for fold 13 saved to model-audio/model_person_12.pth\n",
      "LOOCV Fold 14/38\n",
      "Epoch 50/120: Train Loss = 0.1010\n",
      "Epoch 100/120: Train Loss = 0.0150\n",
      "Fold 14/38 for person 7: True (orig) = [7.], Pred (orig) = [5.2535625]\n",
      "Model for fold 14 saved to model-audio/model_person_7.pth\n",
      "LOOCV Fold 15/38\n",
      "Epoch 50/120: Train Loss = 0.0965\n",
      "Epoch 100/120: Train Loss = 0.0201\n",
      "Fold 15/38 for person 25: True (orig) = [6.85714286], Pred (orig) = [4.7123456]\n",
      "Model for fold 15 saved to model-audio/model_person_25.pth\n",
      "LOOCV Fold 16/38\n",
      "Epoch 50/120: Train Loss = 0.1245\n",
      "Epoch 100/120: Train Loss = 0.0236\n",
      "Fold 16/38 for person 32: True (orig) = [5.85714286], Pred (orig) = [5.8665113]\n",
      "Model for fold 16 saved to model-audio/model_person_32.pth\n",
      "LOOCV Fold 17/38\n",
      "Epoch 50/120: Train Loss = 0.1154\n",
      "Epoch 100/120: Train Loss = 0.0202\n",
      "Fold 17/38 for person 33: True (orig) = [5.], Pred (orig) = [4.7676387]\n",
      "Model for fold 17 saved to model-audio/model_person_33.pth\n",
      "LOOCV Fold 18/38\n",
      "Epoch 50/120: Train Loss = 0.1318\n",
      "Epoch 100/120: Train Loss = 0.0169\n",
      "Fold 18/38 for person 35: True (orig) = [4.57142857], Pred (orig) = [6.310244]\n",
      "Model for fold 18 saved to model-audio/model_person_35.pth\n",
      "LOOCV Fold 19/38\n",
      "Epoch 50/120: Train Loss = 0.1024\n",
      "Epoch 100/120: Train Loss = 0.0332\n",
      "Fold 19/38 for person 8: True (orig) = [6.42857143], Pred (orig) = [6.3308864]\n",
      "Model for fold 19 saved to model-audio/model_person_8.pth\n",
      "LOOCV Fold 20/38\n",
      "Epoch 50/120: Train Loss = 0.1314\n",
      "Epoch 100/120: Train Loss = 0.0301\n",
      "Fold 20/38 for person 41: True (orig) = [6.42857143], Pred (orig) = [5.2219753]\n",
      "Model for fold 20 saved to model-audio/model_person_41.pth\n",
      "LOOCV Fold 21/38\n",
      "Epoch 50/120: Train Loss = 0.1332\n",
      "Epoch 100/120: Train Loss = 0.0240\n",
      "Fold 21/38 for person 18: True (orig) = [1.71428571], Pred (orig) = [2.878665]\n",
      "Model for fold 21 saved to model-audio/model_person_18.pth\n",
      "LOOCV Fold 22/38\n",
      "Epoch 50/120: Train Loss = 0.1228\n",
      "Epoch 100/120: Train Loss = 0.0192\n",
      "Fold 22/38 for person 23: True (orig) = [6.71428571], Pred (orig) = [6.6297393]\n",
      "Model for fold 22 saved to model-audio/model_person_23.pth\n",
      "LOOCV Fold 23/38\n",
      "Epoch 50/120: Train Loss = 0.1138\n",
      "Epoch 100/120: Train Loss = 0.0354\n",
      "Fold 23/38 for person 16: True (orig) = [5.28571429], Pred (orig) = [5.8379664]\n",
      "Model for fold 23 saved to model-audio/model_person_16.pth\n",
      "LOOCV Fold 24/38\n",
      "Epoch 50/120: Train Loss = 0.1276\n",
      "Epoch 100/120: Train Loss = 0.0268\n",
      "Fold 24/38 for person 9: True (orig) = [5.], Pred (orig) = [6.3118525]\n",
      "Model for fold 24 saved to model-audio/model_person_9.pth\n",
      "LOOCV Fold 25/38\n",
      "Epoch 50/120: Train Loss = 0.1091\n",
      "Epoch 100/120: Train Loss = 0.0211\n",
      "Fold 25/38 for person 6: True (orig) = [6.57142857], Pred (orig) = [6.0712647]\n",
      "Model for fold 25 saved to model-audio/model_person_6.pth\n",
      "LOOCV Fold 26/38\n",
      "Epoch 50/120: Train Loss = 0.1127\n",
      "Epoch 100/120: Train Loss = 0.0398\n",
      "Fold 26/38 for person 24: True (orig) = [6.42857143], Pred (orig) = [5.2518554]\n",
      "Model for fold 26 saved to model-audio/model_person_24.pth\n",
      "LOOCV Fold 27/38\n",
      "Epoch 50/120: Train Loss = 0.1230\n",
      "Epoch 100/120: Train Loss = 0.0247\n",
      "Fold 27/38 for person 17: True (orig) = [6.85714286], Pred (orig) = [5.6708565]\n",
      "Model for fold 27 saved to model-audio/model_person_17.pth\n",
      "LOOCV Fold 28/38\n",
      "Epoch 50/120: Train Loss = 0.1383\n",
      "Epoch 100/120: Train Loss = 0.0302\n",
      "Fold 28/38 for person 15: True (orig) = [3.], Pred (orig) = [4.350607]\n",
      "Model for fold 28 saved to model-audio/model_person_15.pth\n",
      "LOOCV Fold 29/38\n",
      "Epoch 50/120: Train Loss = 0.1189\n",
      "Epoch 100/120: Train Loss = 0.0269\n",
      "Fold 29/38 for person 19: True (orig) = [6.], Pred (orig) = [5.592667]\n",
      "Model for fold 29 saved to model-audio/model_person_19.pth\n",
      "LOOCV Fold 30/38\n",
      "Epoch 50/120: Train Loss = 0.1156\n",
      "Epoch 100/120: Train Loss = 0.0369\n",
      "Fold 30/38 for person 36: True (orig) = [6.], Pred (orig) = [5.7342024]\n",
      "Model for fold 30 saved to model-audio/model_person_36.pth\n",
      "LOOCV Fold 31/38\n",
      "Epoch 50/120: Train Loss = 0.1368\n",
      "Epoch 100/120: Train Loss = 0.0298\n",
      "Fold 31/38 for person 22: True (orig) = [4.], Pred (orig) = [4.8147316]\n",
      "Model for fold 31 saved to model-audio/model_person_22.pth\n",
      "LOOCV Fold 32/38\n",
      "Epoch 50/120: Train Loss = 0.1022\n",
      "Epoch 100/120: Train Loss = 0.0161\n",
      "Fold 32/38 for person 37: True (orig) = [2.42857143], Pred (orig) = [4.297848]\n",
      "Model for fold 32 saved to model-audio/model_person_37.pth\n",
      "LOOCV Fold 33/38\n",
      "Epoch 50/120: Train Loss = 0.1275\n",
      "Epoch 100/120: Train Loss = 0.0410\n",
      "Fold 33/38 for person 21: True (orig) = [6.71428571], Pred (orig) = [5.1598573]\n",
      "Model for fold 33 saved to model-audio/model_person_21.pth\n",
      "LOOCV Fold 34/38\n",
      "Epoch 50/120: Train Loss = 0.1015\n",
      "Epoch 100/120: Train Loss = 0.0180\n",
      "Fold 34/38 for person 28: True (orig) = [4.71428571], Pred (orig) = [3.8382394]\n",
      "Model for fold 34 saved to model-audio/model_person_28.pth\n",
      "LOOCV Fold 35/38\n",
      "Epoch 50/120: Train Loss = 0.1061\n",
      "Epoch 100/120: Train Loss = 0.0243\n",
      "Fold 35/38 for person 14: True (orig) = [3.], Pred (orig) = [4.361445]\n",
      "Model for fold 35 saved to model-audio/model_person_14.pth\n",
      "LOOCV Fold 36/38\n",
      "Epoch 50/120: Train Loss = 0.1130\n",
      "Epoch 100/120: Train Loss = 0.0186\n",
      "Fold 36/38 for person 31: True (orig) = [4.14285714], Pred (orig) = [6.0407276]\n",
      "Model for fold 36 saved to model-audio/model_person_31.pth\n",
      "LOOCV Fold 37/38\n",
      "Epoch 50/120: Train Loss = 0.0976\n",
      "Epoch 100/120: Train Loss = 0.0169\n",
      "Fold 37/38 for person 20: True (orig) = [5.42857143], Pred (orig) = [4.817728]\n",
      "Model for fold 37 saved to model-audio/model_person_20.pth\n",
      "LOOCV Fold 38/38\n",
      "Epoch 50/120: Train Loss = 0.1033\n",
      "Epoch 100/120: Train Loss = 0.0397\n",
      "Fold 38/38 for person 10: True (orig) = [4.71428571], Pred (orig) = [6.405018]\n",
      "Model for fold 38 saved to model-audio/model_person_10.pth\n",
      "\n",
      "--- Final LOOCV Results (Original Scale) ---\n",
      "R² Score: 0.3159\n",
      "MSE: 1.2781\n",
      "Correlation: 0.5828\n",
      "P-value: 0.0001\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set seed for reproducibility\n",
    "\"\"\"torch.manual_seed(10_000)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(10_000)\n",
    "np.random.seed(10_000)\"\"\"\n",
    "\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'learning_rate': 1e-3,        # Reduced learning rate\n",
    "    'num_epochs': 120,            # Max epochs (early stopping might trigger)\n",
    "    'batch_size': 37,             # Consider adjusting based on N (e.g., len(train_dataset))\n",
    "    'attn_hidden_dim': 1,        # Increased attention hidden dimension 16\n",
    "    'fc_hidden_dim': 2048,\n",
    "    'weight_decay': 1e-2,         # Added L2 regularization\n",
    "    'dropout_rate': 0.4,          # Dropout probability\n",
    "    'use_dropout': True,          # Flag to enable/disable dropout True\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else\n",
    "              \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "}\n",
    "\n",
    "# Dataset class (Unchanged)\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        # Ensure sequences are tensors\n",
    "        self.sequences = [torch.as_tensor(seq, dtype=torch.float) for seq in sequences]\n",
    "        # Ensure targets are tensors\n",
    "        self.targets = torch.as_tensor(targets, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "# Collate function for variable length sequences (Unchanged)\n",
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    # Ensure sequences are tensors before padding\n",
    "    sequences = [torch.as_tensor(seq, dtype=torch.float) for seq in sequences]\n",
    "    targets = torch.as_tensor(targets, dtype=torch.float)\n",
    "\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0) # Use 0 for padding\n",
    "    \n",
    "    # Create mask: True for padded positions\n",
    "    mask = torch.zeros(padded_sequences.size(0), padded_sequences.size(1), dtype=torch.bool)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if seq.size(0) < padded_sequences.size(1):\n",
    "            mask[i, seq.size(0):] = True\n",
    "\n",
    "    return padded_sequences, targets, mask\n",
    "\n",
    "# Revised model with optional attention and dropout (Unchanged)\n",
    "class SequencePredictor(nn.Module):\n",
    "    def __init__(self, embedding_dim, attn_hidden_dim, fc_hidden_dim, dropout_rate=0.1, use_dropout=True):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "        self.use_dropout = use_dropout\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, attn_hidden_dim),\n",
    "            #nn.Tanh(),\n",
    "            #nn.Linear(attn_hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        fc_layers = [\n",
    "            nn.Linear(embedding_dim, fc_hidden_dim),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        if self.use_dropout:\n",
    "            fc_layers.append(nn.Dropout(dropout_rate)) # Added Dropout\n",
    "\n",
    "        self.fc_hidden = nn.Sequential(*fc_layers)\n",
    "\n",
    "        self.output = nn.Linear(fc_hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: (batch_size, seq_len, embedding_dim)\n",
    "        # mask shape: (batch_size, seq_len), True where padded\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attn_scores = self.attention(x) # (batch_size, seq_len, 1)\n",
    "        if mask is not None:\n",
    "                # mask.unsqueeze(-1) shape: (batch_size, seq_len, 1)\n",
    "            attn_scores = attn_scores.masked_fill(mask.unsqueeze(-1), float('-inf'))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=1) # (batch_size, seq_len, 1)\n",
    "\n",
    "        # Apply attention pooling: sum(weights * features)\n",
    "        # attn_weights * x -> (batch_size, seq_len, embedding_dim)\n",
    "        pooled_output = torch.sum(attn_weights * x, dim=1) # (batch_size, embedding_dim)\n",
    "\n",
    "        hidden_representation = self.fc_hidden(pooled_output) # (batch_size, fc_hidden_dim)\n",
    "        # Store last hidden layer output for later use\n",
    "        # self.last_pooled_output = hidden_representation.cpu().detach()\n",
    "    \n",
    "        # Apply prediction head\n",
    "        # fc output shape: (batch_size, 1) -> squeeze -> (batch_size,)\n",
    "        return self.output(hidden_representation).squeeze(-1) # Squeeze last dim\n",
    "\n",
    "# Revised Training function with early stopping based on train loss (Unchanged)\n",
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs):\n",
    "    model.train() # Set model to training mode (enables dropout)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for sequences, targets, mask in train_loader:\n",
    "            sequences, targets, mask = sequences.to(device), targets.to(device), mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(sequences, mask)\n",
    "            loss = criterion(predictions, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * sequences.size(0)\n",
    "\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Revised Evaluation function (Unchanged)\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval() # Set model to evaluation mode (disables dropout)\n",
    "    true_values_scaled = []\n",
    "    pred_values_scaled = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, targets, mask in data_loader:\n",
    "            sequences, targets, mask = sequences.to(device), targets.to(device), mask.to(device)\n",
    "            predictions = model(sequences, mask)\n",
    "\n",
    "            # Store scaled values\n",
    "            true_values_scaled.extend(targets.cpu().numpy())\n",
    "            pred_values_scaled.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return np.array(true_values_scaled), np.array(pred_values_scaled)\n",
    "\n",
    "# *** MODIFIED: Leave-One-Out Cross Validation with internal y scaling ***\n",
    "def run_loocv(X, y_original, config): # Pass ORIGINAL y\n",
    "    loo = LeaveOneOut()\n",
    "    device = config['device']\n",
    "    all_true_orig = []\n",
    "    all_pred_orig = []\n",
    "\n",
    "    # --- Determine embedding_dim (Unchanged) ---\n",
    "    first_seq = X[0]\n",
    "    if isinstance(first_seq, (list, np.ndarray)):\n",
    "        first_element = first_seq[0]\n",
    "        if isinstance(first_element, (list, np.ndarray)):\n",
    "             embedding_dim = len(first_element)\n",
    "        elif torch.is_tensor(first_element):\n",
    "             embedding_dim = first_element.shape[0]\n",
    "        else:\n",
    "             embedding_dim = 1\n",
    "    elif torch.is_tensor(first_seq):\n",
    "         embedding_dim = first_seq.shape[1]\n",
    "    else:\n",
    "        raise ValueError(\"Could not determine embedding dimension from X\")\n",
    "\n",
    "    fold = 0\n",
    "    total_folds = len(X)\n",
    "    y_indices = np.arange(total_folds) # Use indices for splitting X and y\n",
    "\n",
    "    # Ensure y_original is a numpy array for easier indexing\n",
    "    y_original = np.asarray(y_original)\n",
    "\n",
    "    for train_idx, test_idx in loo.split(y_indices):\n",
    "        fold += 1\n",
    "        print(f\"LOOCV Fold {fold}/{total_folds}\")\n",
    "\n",
    "        # --- Split original data ---\n",
    "        X_train_fold = [np.array(X[i]) if isinstance(X[i], list) else X[i] for i in train_idx]\n",
    "        y_train_fold_orig = y_original[train_idx] # Original y values for training set\n",
    "\n",
    "        X_test_fold = [np.array(X[i]) if isinstance(X[i], list) else X[i] for i in test_idx]\n",
    "        y_test_fold_orig = y_original[test_idx] # Original y value(s) for test set\n",
    "\n",
    "        # --- Scale y INSIDE the loop ---\n",
    "        # Reshape y_train for scaler\n",
    "        y_train_fold_reshaped = y_train_fold_orig.reshape(-1, 1)\n",
    "\n",
    "        # Initialize and fit scaler ONLY on training data for this fold\n",
    "        y_scaler_fold = StandardScaler()\n",
    "        y_train_scaled = y_scaler_fold.fit_transform(y_train_fold_reshaped).flatten()\n",
    "\n",
    "        # Transform test data using the FITTED scaler\n",
    "        # Reshape y_test for transform (even if it's a single value)\n",
    "        y_test_fold_reshaped = y_test_fold_orig.reshape(-1, 1)\n",
    "        y_test_scaled = y_scaler_fold.transform(y_test_fold_reshaped).flatten()\n",
    "        # ----------------------------------\n",
    "\n",
    "        # Create datasets and dataloaders using SCALED y for this fold\n",
    "        train_dataset = SequenceDataset(X_train_fold, y_train_scaled)\n",
    "        # Use the scaled test value for the test dataset\n",
    "        test_dataset = SequenceDataset(X_test_fold, y_test_scaled)\n",
    "\n",
    "        # --- Dynamic Batch Size (Unchanged) ---\n",
    "        train_batch_size = min(config['batch_size'], len(train_dataset))\n",
    "        if len(train_dataset) == 0:\n",
    "             print(f\"Warning: Fold {fold} has an empty training set. Skipping.\")\n",
    "             continue\n",
    "        if train_batch_size == 0: train_batch_size = 1\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=1, # Keep batch size 1 for test in LOOCV\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        # --- Model Initialization (Unchanged) ---\n",
    "        model = SequencePredictor(\n",
    "            embedding_dim=embedding_dim,\n",
    "            attn_hidden_dim=config['attn_hidden_dim'],\n",
    "            fc_hidden_dim=config['fc_hidden_dim'],\n",
    "            dropout_rate=config['dropout_rate'],\n",
    "            use_dropout=config['use_dropout']\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "        )\n",
    "        criterion = nn.MSELoss() # Use MSE for scaled targets\n",
    "\n",
    "        # --- Train Model (Unchanged) ---\n",
    "        model = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            device=device,\n",
    "            num_epochs=config['num_epochs'],\n",
    "        )\n",
    "\n",
    "        # --- Evaluate Model (gets scaled predictions) ---\n",
    "        # evaluate_model returns the ground truth scaled y from the dataloader\n",
    "        # and the model's predictions (also scaled)\n",
    "        _, pred_vals_scaled = evaluate_model(model, test_loader, device)\n",
    "\n",
    "        # --- Inverse transform predictions and store original values ---\n",
    "        # We already have the original true value: y_test_fold_orig\n",
    "        # Inverse transform predictions using the scaler fitted for THIS FOLD\n",
    "        pred_vals_orig = y_scaler_fold.inverse_transform(pred_vals_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "        print(f\"Fold {fold}/{total_folds} for person {person_ids[test_idx[0]]}: True (orig) = {y_test_fold_orig}, Pred (orig) = {pred_vals_orig}\")\n",
    "\n",
    "        # Store original true value(s) and original-scale prediction(s)\n",
    "        all_true_orig.extend(y_test_fold_orig) # Use the original test value\n",
    "        all_pred_orig.extend(pred_vals_orig)   # Use the inverse-transformed prediction\n",
    "\n",
    "        os.makedirs(\"model-audio\", exist_ok=True) # Ensure directory exists\n",
    "        # Save the model\n",
    "        model_save_path = f\"model-audio/model_person_{person_ids[test_idx[0]]}.pth\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model for fold {fold} saved to {model_save_path}\")\n",
    "\n",
    "    # --- Compute final metrics (Unchanged section, uses collected orig values) ---\n",
    "    all_true_orig = np.array(all_true_orig)\n",
    "    all_pred_orig = np.array(all_pred_orig)\n",
    "\n",
    "    valid_indices = np.isfinite(all_pred_orig)\n",
    "    if not np.all(valid_indices):\n",
    "        print(f\"Warning: Found {np.sum(~valid_indices)} non-finite predictions. Evaluating metrics only on finite predictions.\")\n",
    "        all_true_orig = all_true_orig[valid_indices]\n",
    "        all_pred_orig = all_pred_orig[valid_indices]\n",
    "\n",
    "    if len(all_true_orig) < 2:\n",
    "         print(\"Warning: Less than 2 valid prediction pairs. Cannot calculate metrics.\")\n",
    "         results = {\n",
    "             'r2': np.nan, 'mse': np.nan, 'correlation': np.nan, 'p_value': np.nan,\n",
    "             'true_values_orig': all_true_orig.tolist(), 'predicted_values_orig': all_pred_orig.tolist()\n",
    "         }\n",
    "    else:\n",
    "        r2 = r2_score(all_true_orig, all_pred_orig)\n",
    "        mse = mean_squared_error(all_true_orig, all_pred_orig)\n",
    "        corr, p_value = pearsonr(all_true_orig.flatten(), all_pred_orig)\n",
    "        # --- Print metrics (Unchanged) ---\n",
    "        results = {\n",
    "            'r2': r2, 'mse': mse, 'correlation': corr, 'p_value': p_value,\n",
    "            'true_values_orig': all_true_orig.tolist(), 'predicted_values_orig': all_pred_orig.tolist()\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# *** MODIFIED: Main execution function ***\n",
    "def main(X, y, config=None):\n",
    "    if config is None:\n",
    "        config = CONFIG\n",
    "\n",
    "    print(f\"Using device: {config['device']}\")\n",
    "    print(f\"Configuration: {config}\")\n",
    "\n",
    "    # --- Data Scaling for y REMOVED from here ---\n",
    "    # Ensure y is suitable for passing (e.g., list or numpy array)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # Optional: Scale X features here if necessary (remains unchanged)\n",
    "    print(\"Input sequences 'X' are assumed to be appropriately scaled/normalized outside the LOOCV loop.\")\n",
    "    print(\"Target variable 'y' will be scaled within each LOOCV fold.\")\n",
    "\n",
    "    # --- Run LOOCV ---\n",
    "    # Pass the ORIGINAL y to run_loocv\n",
    "    results = run_loocv(X, y, config) # Pass original y\n",
    "\n",
    "    # --- Print Results (Unchanged) ---\n",
    "    print(\"\\n--- Final LOOCV Results (Original Scale) ---\")\n",
    "    if np.isnan(results['r2']):\n",
    "         print(\"Metrics could not be calculated (too few valid predictions).\")\n",
    "    else:\n",
    "        print(f\"R² Score: {results['r2']:.4f}\")\n",
    "        print(f\"MSE: {results['mse']:.4f}\")\n",
    "        print(f\"Correlation: {results['correlation']:.4f}\")\n",
    "        print(f\"P-value: {results['p_value']:.4f}\")\n",
    "\n",
    "    # print(f\"True values (original): {results['true_values_orig']}\")\n",
    "    # print(f\"Predicted values (original): {results['predicted_values_orig']}\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'Person ID': person_ids,\n",
    "        'True Values': [round(result, 2) for result in results['true_values_orig']],\n",
    "        'Predicted Values': [round(result, 2) for result in results['predicted_values_orig']]\n",
    "    })\n",
    "    # Order by Person ID\n",
    "    results_df.sort_values(by='Person ID', inplace=True)\n",
    "    results_df.to_csv('predictions-text-attention.csv', index=False)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage (replace with your actual data)\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # --- Run the main function ---\n",
    "    results = main(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f587c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b11fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
