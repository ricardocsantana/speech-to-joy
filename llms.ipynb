{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a dictionary to store the LLM predictions\n",
    "llm_predictions = {}\n",
    "\n",
    "user_reports_df = pd.read_excel(\"user-self-reports/target-enjoyment.xlsx\", sheet_name='target-enjoyment', header=1)\n",
    "\n",
    "# Loop through all the LLM prediction files in the Backup-gemini-2.5-pro directory\n",
    "for filename in os.listdir(\"./llm-gemini-2.o-flash-predictions\"):\n",
    "    if not filename.endswith(\"-LLM-pred.txt\"):\n",
    "        continue\n",
    "    \n",
    "    # Extract participant ID from filename\n",
    "    participant_id = int(re.search(r'P(\\d+)', filename).group(1))\n",
    "    \n",
    "    # Read the file content\n",
    "    with open(f\"./llm-gemini-2.o-flash-predictions/{filename}\", \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split the content by robot\n",
    "    robot_sections = content.split(\"Robot: \")\n",
    "    \n",
    "    for section in robot_sections[1:]:  # Skip the first empty section\n",
    "        lines = section.strip().split(\"\\n\", 1)\n",
    "        if len(lines) < 2:\n",
    "            continue\n",
    "            \n",
    "        robot_name = lines[0]\n",
    "\n",
    "        if robot_name != user_reports_df.loc[user_reports_df['PID'] == participant_id, 'Q1-Robot'].values[0]:\n",
    "            continue\n",
    "\n",
    "        # Extract the response text\n",
    "        response_text = lines[1].replace(\"Response: \", \"\")\n",
    "        \n",
    "        # Try to parse the JSON response\n",
    "        try:\n",
    "            # Extract JSON part if it exists\n",
    "            json_match = re.search(r'(\\{.*\\})', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                response_json = json.loads(json_match.group(1))\n",
    "                \n",
    "                # Create a dictionary for this participant and robot if it doesn't exist\n",
    "                if participant_id not in llm_predictions:\n",
    "                    llm_predictions[participant_id] = {}\n",
    "                \n",
    "                # Store the predictions\n",
    "                llm_predictions[participant_id][robot_name] = response_json\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response for P{participant_id}, {robot_name}: {e}\")\n",
    "\n",
    "# Create a list to store the rows for the DataFrame\n",
    "llm_data = []\n",
    "\n",
    "# Iterate through each participant and their robot data\n",
    "for participant_id, robots in llm_predictions.items():\n",
    "    for robot, predictions in robots.items():\n",
    "        # Create a row for this participant and robot\n",
    "        row = {\n",
    "            'user_id': participant_id,\n",
    "            'robot': robot\n",
    "        }\n",
    "        \n",
    "        # Add the predictions for each question\n",
    "        for q_num in range(1, 9):\n",
    "            question_key = f\"Question {q_num}\"\n",
    "            if question_key in predictions:\n",
    "                # For Q7 and Q8, we need to invert the scale (8 - value) to match the original data\n",
    "                if q_num == 7 or q_num == 8:\n",
    "                    row[f\"Q{q_num}\"] = 8 - predictions[question_key]\n",
    "                else:\n",
    "                    row[f\"Q{q_num}\"] = predictions[question_key]\n",
    "            else:\n",
    "                row[f\"Q{q_num}\"] = None\n",
    "        \n",
    "        llm_data.append(row)\n",
    "\n",
    "# Create the DataFrame\n",
    "llm_df = pd.DataFrame(llm_data)\n",
    "\n",
    "# Calculate the average score for each participant and robot\n",
    "llm_df[\"Average\"] = llm_df.iloc[:, 2:10].mean(axis=1)\n",
    "\n",
    "# Sort by user_id to make it easier to read\n",
    "llm_df = llm_df.sort_values(by=['user_id', 'robot']).reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(llm_df.head(10))\n",
    "print(f\"Total number of records: {len(llm_df)}\")\n",
    "\n",
    "# Create a figure with bigger size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a distribution plot of the averages by robot type\n",
    "ax = sns.histplot(data=llm_df, x='Average', hue='robot', kde=True, bins=10, \n",
    "                 element='step', palette=['blue', 'red'], alpha=0.5)\n",
    "\n",
    "# Add a vertical line for the mean of each robot\n",
    "for robot, color in zip(['Alice', 'Clara'], ['blue', 'red']):\n",
    "    mean_val = llm_df[llm_df['robot'] == robot]['Average'].mean()\n",
    "    plt.axvline(x=mean_val, color=color, linestyle='--', \n",
    "                label=f'Mean for {robot}: {mean_val:.2f}')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of LLM-Predicted Average Ratings by Robot', fontsize=16)\n",
    "plt.xlabel('Average Rating', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0139974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save user id and average to a new CSV file\n",
    "llm_df[[\"user_id\", \"Average\"]].to_csv('predictions-llm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Merge the dataframes to compare LLM predictions with actual user ratings\n",
    "# First, let's make sure the ordering is consistent by setting index as user_id and robot\n",
    "ordered_df_indexed = ordered_df.set_index(['user_id', 'robot'])\n",
    "llm_df_indexed = llm_df.set_index(['user_id', 'robot'])\n",
    "\n",
    "# Find the common indices (user_id, robot combinations present in both datasets)\n",
    "common_indices = ordered_df_indexed.index.intersection(llm_df_indexed.index)\n",
    "\n",
    "# Filter both dataframes to only include common indices\n",
    "ordered_filtered = ordered_df_indexed.loc[common_indices]\n",
    "llm_filtered = llm_df_indexed.loc[common_indices]\n",
    "\n",
    "# Calculate MSE for average ratings\n",
    "mse = mean_squared_error(ordered_filtered['Average'], llm_filtered['Average'])\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate MSE, correlation, and p-value for each question\n",
    "question_mse = {}\n",
    "question_correlation = {}\n",
    "question_p_values = {}\n",
    "for q in range(1, 8):\n",
    "    q_col = f'Q{q}'\n",
    "    q_mse = mean_squared_error(ordered_filtered[q_col], llm_filtered[q_col])\n",
    "    question_mse[q_col] = q_mse\n",
    "\n",
    "    # Calculate correlation and p-value for the question\n",
    "    correlation, p_value = pearsonr(ordered_filtered[q_col], llm_filtered[q_col])\n",
    "    question_correlation[q_col] = correlation\n",
    "    question_p_values[q_col] = p_value\n",
    "\n",
    "# Calculate correlation and p-value for average ratings\n",
    "average_correlation, average_p_value = pearsonr(ordered_filtered['Average'], llm_filtered['Average'])\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean Squared Error (MSE) between LLM and user average ratings: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Correlation between LLM and user average ratings: {average_correlation:.4f}\")\n",
    "print(f\"P-value for average correlation: {average_p_value:.4e}\")\n",
    "print(\"\\nMSE for each question:\")\n",
    "for q, q_mse in question_mse.items():\n",
    "    print(f\"{q}: {q_mse:.4f}\")\n",
    "\n",
    "print(\"\\nCorrelation and P-value for each question:\")\n",
    "for q in range(1, 8):\n",
    "    print(f\"{q}: Correlation = {question_correlation[f'Q{q}']:.4f}, P-value = {question_p_values[f'Q{q}']:.4e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
